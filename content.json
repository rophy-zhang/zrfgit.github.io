{"meta":{"title":"飞的博客","subtitle":null,"description":"知识的积累，随心笔记。","author":"飞","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"MapReduce","slug":"MapReduce","date":"2019-04-20T16:00:00.000Z","updated":"2019-04-21T12:56:32.459Z","comments":true,"path":"2019/04/21/MapReduce/","link":"","permalink":"http://yoursite.com/2019/04/21/MapReduce/","excerpt":"","text":"一、分布式计算思想​ 随着大量设备的联网，我们能够获取的数据越来越多，通过数据都是需要进行决策处理，但通常这些数据量太庞大了，超出了我们单台机器所能处理的范围。问题会有的，解决方案也会有的，一个开源项目Hadoop，是用Java编写，支持大量机器上分布式处理数据。 1.分布式计算框架（MapReduce）​ MapReduce是一个软件框架，可以在单台计算机上把任务分配给多台计算机来执行，这样就可以缩短运行时间。它的工作流程是：单个作业被分成很多小份，输入的数据也被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为mapper,这个过程是map阶段。每个mapper的输出通过某种方式组合排序，结果再划分成小份分发到各个节点下进行处理，这一步是reduce阶段，被称为reducer，reducer的输出就是最终结果。 ​ MapReduce可以并行执行，本来十个小时的任务分给十台机器后一个多小时就完成了。比如要知道中国过去100年国内最高气温，其中数据格式是，我们可以先将数据切分成很多分，每个节点找出本机数据集中最高的气温，这样每个mapper产生一个温度，比如：&lt;”max”&gt;，也就是所有的mapper都会产生相同的key：“max”字符串。最后通过一个reducer来比较所有mapper的输出就可以得到全局最高气温。 ​ 在MapReduce整个工作中，编配工作由主节点控制，主节点控制整个MapReduce的作业分配，包括每份数据存放节点位置，和map，sort，reduce的时序控制。一般每份mapper的输入数据会同时分发到多个节点，形成多个副本，用于事务失效处理。 2.MapReduce上的机器学习​ 在机器学习算法中很多不能直接用MapReduce来实现，一般贝叶斯、k近邻算法、SVM、SVD、k-Means可以用MapReduce直接实现。 ​ 在Python中mrjob这个来自动化MapReduce。我们可以利用这个库来实现我们的分布式机器学习算法。 二、分布式SVM实践​ 这里我们采用Pegasos算法优化SVM来实现分布式SVM。其中工作流程是：从训练集中随机挑选一些样本点添加到待处理列表，之后判断每个样本点是否分类正确，如果正确则忽略，否则加入到待更新集合。批处理完毕后，权重向量按照这些错分的样本进行更新。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970mport picklefrom numpy import *class MRsvm(MRJob): DEFAULT_INPUT_PROTOCOL = 'json_value' def __init__(self,*args,**kwargs): super(MRsvm,self).__init__(*args,**kwargs) self.data = pickle.load(open('D:\\学习资源\\机器学习\\ML\\data\\Ch15\\svmDat27')) self.w = 0 self.eta = 0.69 self.dataList = [] self.k = self.options.batchsize self.numMappers = 1 self.t = 1 def configure_options(self): super(MRsvm,self).configure_options() self.add_passthrough_option('--iterations',dest='iterations',default=2,type='int',help='T:number of iterations to run') self.add_passthrough_option('--batchsize',dest='batchsize',default=100,type='int',help='k:number of data points in a batch') def steps(self): return ([MRStep(mapper=self.map,mapper_final=self.map_fin,reducer=self.reduce)]*self.options.iterations) def map(self,mapperId,inVals): if False: yield if inVals[0] == 'w': self.w = inVals[1] elif inVals[0] == 'x': self.dataList.append(inVals[1]) elif inVals[0] == 't': self.t = inVals[1] def map_fin(self): labels = self.data[:,-1] X = self.data[:,0:-1] if self.w == 0: self.w = [0.001]*shape(X)[1] for index in self.dataList: p = mat(self.w)*X[index,:].T if labels[index]*p &lt; 1.0: yield (1,['u',index]) yield (1,['w',self.w]) yield (1,['t',self.t]) def reduce(self,_,packedVals): for valArr in packedVals: if valArr[0] == 'u': self.dataList.append(valArr[1]) elif valArr[0] == 'w': self.w = valArr[1] elif valArr[0] == 't': self.t = valArr[1] labels = self.data[:,-1] X = self.data[:,0:-1] wMat = mat(self.w) wDelta = mat(zeros(len(self.w))) for index in self.dataList: wDelta += float(labels[index])*X[index,:] eta = 1.0/(2.0*self.t) wMat = (1.0 - 1.0/self.t)*wMat + (eta/self.k)*wDelta for mapperNum in range(1,self.numMappers+1): yield (mapperNum,['w',wMat.tolist()[0]]) if self.t &lt; self.options.iterations: yield (mapperNum,['t',self.t+1]) for j in range(self.k/self.numMappers): yield (mapperNum,['x',random.randint(shape(self.data)[0])])if __name__ == '__main__': MRsvm.run() 三、后记​ 通过分布式方式进行处理，我们可以把大量数据计算分不到各个节点达到并行处理。","categories":[],"tags":[]},{"title":"奇异值分解（Singular Value Decomposition）","slug":"SVD","date":"2019-04-19T16:00:00.000Z","updated":"2019-04-21T03:40:56.795Z","comments":true,"path":"2019/04/20/SVD/","link":"","permalink":"http://yoursite.com/2019/04/20/SVD/","excerpt":"","text":"一、算法理论1.SVD算法剧情介绍​ SVD奇异值分解可以把复杂的数据进行分解成简单的数据集来表示原始数据、去除原始数据噪声，提取其中的主要信息，对数据进行压缩，找出其中的影响因素。 ​ 其中在隐性语义索引中，SVD可以把文档词语矩阵进行分解，找出其中的奇异值，这些奇异值就代表了文档的概念和主题，运用这一点可以提高搜索效率。比如在推荐系统中通过SVD可以构造出一个主题空间，然后再计算事务之间的相似度。假如有一家餐馆的菜由品菜师进行了打分，我们可以对品菜师和菜构成的矩阵进行奇异值分解，得到其中的奇异值，我们可以从中得出这些品菜师对于哪类菜更喜爱。 2.获取相关数据​ 这里的数据中的数字表示品菜师对菜的评分，0表示没有评分，1-5为评分。 品菜师\\菜类 鳗鱼饭 日式炸鸡排 寿司饭 烤牛肉 手撕猪肉 Ed 0 0 0 2 2 Peter 0 0 0 3 3 Tracy 0 0 0 1 1 Fan 1 1 1 0 0 Ming 2 2 2 0 0 Pachi 5 5 5 0 0 Jocalyn 1 1 1 0 0 3.公式原理​ 平时我们对矩阵进行分解时都是对方阵进行特征分解，找到特征值和特征向量，但是有时候矩阵并不是方阵，这时候就要用到SVD了。SVD的分解可以表示为：$$Data_{m\\times n}=U_{m\\times n}\\sum.{m\\times n}V^{T}{n\\times n}$$这里分解会构建出sigma矩阵，这个矩阵是对角元素为奇异值，其余元素为0的对角矩阵，且奇异值从大到小排列。上一节中的PCA技术是得到矩阵的特征值，这里可以得到数据集中的重要特征，其中奇异值是Data数据集矩阵的特征值的平方根。 ​ 我们可以举例说明：我们对矩阵A进行奇异值分解。$$A=\\begin{pmatrix}0 &amp;1 \\1 &amp;1 \\1 &amp;0\\end{pmatrix}$$我们先计算下边两个式子：$$A^TA=\\begin{pmatrix}0 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 0\\end{pmatrix}\\begin{pmatrix}0 &amp;1 \\1 &amp;1 \\1 &amp;0\\end{pmatrix}=\\begin{pmatrix}2 &amp;1 \\1 &amp;2\\end{pmatrix}$$ $$AA^T=\\begin{pmatrix}0 &amp;1 \\1 &amp;1 \\1 &amp;0\\end{pmatrix}\\begin{pmatrix}0 &amp; 1 &amp; 1\\1 &amp; 1 &amp; 0\\end{pmatrix}=\\begin{pmatrix}1 &amp;1 &amp;0\\1 &amp;2 &amp;1\\0 &amp;1 &amp;1\\end{pmatrix}$$ 我们可以求出上边两个式子的特征向量，A^TA：$$\\lambda {1}=3;v{1}=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\frac{1}{\\sqrt{2}}\\end{pmatrix};\\lambda {2}=1;v{2}=\\begin{pmatrix}-\\frac{1}{\\sqrt{2}}\\\\frac{1}{\\sqrt{2}}\\end{pmatrix}$$AA^T的特征值与特征向量：$$\\lambda {1}=3;u{1}=\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\frac{2}{\\sqrt{6}}\\\\frac{1}{\\sqrt{6}}\\end{pmatrix};\\lambda {2}=1;u{2}=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\0\\-\\frac{1}{\\sqrt{2}}\\end{pmatrix};\\lambda {3}=0;u{3}=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\-\\frac{1}{\\sqrt{3}}\\\\frac{1}{\\sqrt{3}}\\end{pmatrix};$$我们可以利用Aui=σiui,i=1,2求解奇异值：$$\\begin{pmatrix}0 &amp;1 \\1 &amp;1 \\1 &amp;0\\end{pmatrix}\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\frac{1}{\\sqrt{2}}\\end{pmatrix}=\\sigma _{1}\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\frac{2}{\\sqrt{6}}\\\\frac{1}{\\sqrt{6}}\\end{pmatrix}\\Rightarrow\\sigma _{1}=\\sqrt{3}$$ $$\\begin{pmatrix}0 &amp;1 \\1 &amp;1 \\1 &amp;0\\end{pmatrix}\\begin{pmatrix}-\\frac{1}{\\sqrt{2}}\\\\frac{1}{\\sqrt{2}}\\end{pmatrix}=\\sigma _{2}\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\0\\-\\frac{1}{\\sqrt{2}}\\end{pmatrix}\\Rightarrow\\sigma _{2}=1$$ 我们也可以利用下式求出奇异值：$$\\sigma _{i}=\\sqrt{\\lambda {i}}$$最终的奇异值分解为：$$A=U\\sum V^{T}=\\begin{pmatrix}\\frac{1}{\\sqrt{6}} &amp;\\frac{1}{\\sqrt{2}} &amp;\\frac{1}{\\sqrt{3}}\\\\frac{2}{\\sqrt{6}} &amp;0 &amp;-\\frac{1}{\\sqrt{3}}\\\\frac{1}{\\sqrt{6}} &amp;-\\frac{1}{\\sqrt{2}} &amp;\\frac{1}{\\sqrt{3}}\\end{pmatrix}\\begin{pmatrix}\\sqrt{3} &amp; 0\\0 &amp; 1\\0 &amp; 0\\end{pmatrix}\\begin{pmatrix}\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}} \\-\\frac{1}{\\sqrt{2}} &amp; \\frac{1}{\\sqrt{2}}\\end{pmatrix}$$奇异值和特征分解的特征值类似，奇异值按照从大到小排列，而且奇异值减小的很快，一般10%到1%的奇异值就占了全部奇异值之和的99%以上的比例。我们就可以取前k个奇异值和对应的左右奇异向量来近似描述原始矩阵。我们可以表示为：$$A{m\\times n}=U_{m\\times m}\\sum.{m\\times n}V{n\\times n}^{T}\\approx U_{m\\times k}\\sum .{k\\times k}V{k\\times n}^{T}$$其中这里的k就是前k个奇异值，可以覆盖90%以上的奇异值之和，而且这里的k很小。这样，PCA就可以运用SVD进行降维，数据压缩去噪。 ​ 在PCA中我们降维采用的是求协方差矩阵A^T*A的特征值和特征向量，然后取前k个最大的特征值对应的特征向量构成的矩阵进行降维，但是当样本数目过多的时候，计算这种方法计算量就会很大。我们可以看到SVD中也可以得到协方差矩阵的k个最大特征向量构成的矩阵，但是SVD可以不求协方差矩阵就可以计算出右奇异矩阵V，则此时我们就没有做特征分解，因此在样本量很大的时候PCA采用SVD分解更好。 ​ 这里还要注意的是，PCA中仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，左奇异矩阵有什么用？我们可以对mxn的矩阵X通过协方差矩阵进行特征分解，得出前k个最大特征向量构成的mxk的矩阵U,我们可以进行以下处理：$$X^{‘}{k\\times n} = U^{T}{k\\times m}X_{m\\times n}$$我们可以得到一个kxn的矩阵X’，这个矩阵比原来的矩阵从m行减小到d行，对行数据进行了压缩，则做奇异矩阵是对行矩阵压缩，右奇异矩阵是对列数据压缩，这就是PCA降维。 4.数据压缩​ 我们可以对非方阵进行奇异值分解得到三个矩阵其中矩阵U是对原始行数据压缩，V^T矩阵是对列矩阵进行压缩，sigma矩阵是选择其中占有90%奇异值的压缩数目，这样我们就可以对原始数据的行和列进行压缩。 二、算法实践1.基于协同过滤的推荐引擎​ 通过推荐引擎我们可以推荐购物物品，推荐看的电影和推荐新闻广告等等，这里我们用协同过滤算法来实现推荐功能。 ​ 协同过滤是通过将用户和其他用户的数据进行对比来实现推荐。我们可以通过计算两个用户或者两个物品之间的相似度，然后来预测未知用户的喜好。比如，我们可以对某个用户喜欢的电影进行预测，假如推荐引擎发现某部电影该用户还未看，然后推荐引擎就会计算这部电影和该用户看过电影之间的相似度，如果相似度很高，推荐引擎就会推荐这部电影。 ​ 我们在推荐引擎中使用的是用户对物品的评价来进行相似度计算推荐的，所以我们获取到三个人对这几道菜的评价信息： 用户\\菜 鳗鱼饭 日式炸鸡排 寿司饭 烤牛肉 手撕猪肉 Jim 2 0 0 4 4 John 5 5 5 3 3 Sally 2 4 2 1 2 我们一般计算相似度采用欧氏距离，我们可以计算手撕猪肉和烤牛肉之间距离：$$\\sqrt{(4-4)^{2}+(3-3)^{2}+(2-1)^{2}}=1$$手撕猪肉和鳗鱼饭之间的欧氏距离：$$\\sqrt{(4-2)^{2}+(3-5)^{2}+(2-2)^{2}}=2.83$$我们这里得出手撕猪肉和烤牛肉之间的距离比手撕猪肉和鳗鱼饭之间的距离更小，所以手撕猪肉与烤牛肉之间更相似。我们可以把值控制到（0,1），然后计算相似度：$$相似度=\\frac{1}{1+距离}$$通过皮尔逊相关系数求解相似度，这个方法有一个优势是对用户的评级量级不敏感，比如有的用户评分5分，有的评分1分，皮尔逊相关系数会认为这种情况是等价的。皮尔逊相关系数取值在（-1，1），因此我们计算：$$0.5+0.5\\times corrcoef()$$还可以采用余弦相似度来计算相似度，余弦相似度的取值范围也是（-1，1），因此我们可以将它归一化到（0,1）：$$\\cos \\Theta =\\frac{A\\cdot B}{\\left | A \\right |\\cdot \\left | B \\right |}$$一般在推荐引擎中我们要计算两个餐馆菜肴的相似度这是基于物品的相似度，还有就是通过用户的相似度来计算是用户相似度。一帮情况下我们会考虑用户数量和物品数量，如果用户数目多我们就采用物品进行相似度计算。 ​ 在推荐引擎评价中，我们用的方法是把已有的部分评分去掉，然后用推荐算法进行预测，然后计算与真实值之间的误差（最小均方根误差） ​ 接下来一个实例就是用推荐引擎推荐餐馆的食物，假如一家人出去旅游，不知道到哪家餐馆和点什么菜，那我们的推荐引擎就可以帮助寻找合适的餐馆并且推荐适当的菜。我们的思路是： （1）寻找用户没有评级的菜肴，在用户物品矩阵中值为0。 （2）我们可以对没有评级的菜肴中预测出一个评分，我们可以利用相似度计算。 （3）对评分的菜肴从高到低排序，选择前N个进行推荐。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118from numpy import *from numpy import linalg as ladef loadExData(): return[[0, 0, 0, 2, 2], [0, 0, 0, 3, 3], [0, 0, 0, 1, 1], [1, 1, 1, 0, 0], [2, 2, 2, 0, 0], [5, 5, 5, 0, 0], [1, 1, 1, 0, 0]] def loadExData2(): return[[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5], [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3], [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0], [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0], [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0], [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0], [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1], [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4], [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2], [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0], [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]]# 相似度计算def ecludSim(inA,inB): return 1.0/(1.0 + la.norm(inA - inB))def pearsSim(inA,inB): if len(inA) &lt; 3: return 1.0 return 0.5+0.5*corrcoef(inA,inB,rowvar = 0)[0][1]def cosSim(inA,inB): num = float(inA.T*inB) denom = la.norm(inA)*la.norm(inB) return 0.5+0.5*(num/denom)'''用户对物品的估计评分值dataMat:数据矩阵user:用户编号simMeas:相似度计算方法item:物品编号'''def standEst(dataMat,user,simMeas,item): n = shape(dataMat)[1] simTotal = 0.0 ratSimTotal = 0.0 for j in range(n): userRating = dataMat[user,j] if userRating == 0: continue # 寻找两个用户都评级的物品 overLap = nonzero(logical_and(dataMat[:,item].A&gt;0,dataMat[:,j].A&gt;0))[0] if len(overLap) == 0: similarity = 0 else: similarity = simMeas(dataMat[overLap,item],dataMat[overLap,j]) #print(\"the %d and %d similarity is:%f\" % (item,j,similarity)) simTotal += similarity ratSimTotal += similarity * userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotal# 推荐引擎def recommend(dataMat,user,N=3,simMeas=cosSim,estMethod=standEst): # 寻找未评级的物品 unratedItems = nonzero(dataMat[user,:].A==0)[1] if len(unratedItems) == 0: return \"you rated everything\" itemScores = [] for item in unratedItems: estimatedScore = estMethod(dataMat,user,simMeas,item) itemScores.append((item,estimatedScore)) # 寻找前N个未评级物品 return sorted(itemScores,key=lambda jj:jj[1],reverse=True)[:N]'''用于评分估计值dataMat:数据集user:用户simMeas:相似度计算方法item:物品项'''def svdEst(dataMat,user,simMeas,item): n = shape(dataMat)[1] simTotal = 0.0 ratSimTotal = 0.0 U,Sigma,VT = la.svd(dataMat) # 建立对角矩阵 Sig4 = mat(eye(4)*Sigma[:4]) # 构建转换后的物品 xformedItems = dataMat.T * U[:,:4] * Sig4.I for j in range(n): userRating = dataMat[user,j] if userRating == 0 or j == item: continue similarity = simMeas(xformedItems[item,:].T,xformedItems[j,:].T) print(\"the %d and %d similarity is:%f\" % (item,j,similarity)) simTotal += similarity ratSimTotal += similarity*userRating if simTotal == 0: return 0 else: return ratSimTotal/simTotalprint(recommend(mat(loadExData2()),1,estMethod=svdEst)) 2.SVD图像压缩​ 这里我们对手写识别里边的数字5进行压缩。 123456789101112131415161718192021222324252627282930313233from numpy import *from numpy import linalg as la'''图像压缩'''def printMat(inMat,thresh=0.8): for i in range(32): for k in range(32): if float(inMat[i,k]) &gt; thresh: print(1) else: print(0) print(\" \")def imgCompress(numSV=3,thresh=0.8): myl = [] for line in open('./data/Ch14/0_5.txt'): newRow = [] for i in range(32): newRow.append(int(line[i])) myl.append(newRow) myMat = mat(myl) print(\"****original matrix******\") printMat(myMat,thresh) U,Sigma,VT = la.svd(myMat) SigRecon = mat(zeros((numSV,numSV))) for k in range(numSV): SigRecon[k,k] = Sigma[k] reconMat = U[:,:numSV]*SigRecon*VT[:numSV,:] print(\"*****reconstructed matrix using %d singular values******\" % numSV) print(reconMat,thresh)imgCompress(2) 三、后记​ 通过SVD我们可以进行快速PCA降维，去除数据噪声等，实现数据压缩，我们接下来还可以运用分布式处理前边所学过的算法，就用到了MapReduce，在大量数据中如何去分布式训练。","categories":[],"tags":[]},{"title":"主成分分析PCA(Principl Component Analysis)","slug":"PCA","date":"2019-04-18T15:39:00.000Z","updated":"2019-04-18T15:23:48.621Z","comments":true,"path":"2019/04/18/PCA/","link":"","permalink":"http://yoursite.com/2019/04/18/PCA/","excerpt":"","text":"一、算法理论1.PCA算法剧情介绍​ 主成分分析技术可以帮助我们把数据中含有大量维度的特征维度进行降维，比如说，人们在观看球赛的时候电视屏幕有百万的像素，然而人们只关注足球的位置，因此人的大脑会筛选像素呀，就缩小到球大小的维度，这就是达到了降维的效果。 ​ 在前边的机器学习中，我们可以运用PCA技术对现有的数据特征进行降维压缩，除去噪声，减小计算开销，使得构造出的新特征之间不相关，消除特征之间的相关性。 2.获取相关数据​ 这里我们利用半导体制造数据，含有590个特征。 序号 特征1 … 特征589 特征590 1 3030.93 … NaN NaN 2 3095.78 … 0.006 208.2045 … … … … … 1566 2894.92 0.0075 93.4941 1567 2944.92 … 0.0045 137.7844 3.公式原理​ 在PCA中主要是将数据原来的坐标系转换到新的坐标系，其中第一个坐标轴选择原始数据中方差最大的方向，第二个坐标轴选择和第一个坐标轴正交且方差次大的方向，以此类推，重复次数为原始数据特征的数目。最后我们发现大部分方差会包含在前边的几个新坐标轴中。这时候我们就可以忽略剩下的坐标轴，这就达到了降维的效果。 ​ 通过数据集的协方差矩阵和特征值分析，我们就可以求得这些主成分的值，我们得到协方差的特征向量，就可以保留最大的N值，这些特征向量就给出了N个最重要的特征的真实结构，我们可以将数据乘上这N个特征向量就将它转换到新的空间了，达到降维效果。 4.特征降维​ 我们通过求解矩阵的协方差矩阵，然后求解协方差矩阵的特征值与特征向量，我们对特征值从大到小排序，选择前N个特征值，然后和原数据相乘就可以的到新的空间中降维后的数据。 二、算法实践​ 在算法实践过程中我们有两点需要注意，这里存在空值，我们处理的方式是用平均值来进行代替，并且需要把特征数据归一化，达到每个特征的公平性一致。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950from numpy import *import matplotlibimport matplotlib.pyplot as pltdef loadDataSet(fileName,delim='\\t'): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [list(map(float,line)) for line in stringArr] return mat(datArr)def pca(dataMat,topNfeat=9999999): meanVals = mean(dataMat,axis=0) # 取平均值 meanRemoved = dataMat - meanVals covMat = cov(meanRemoved,rowvar=0) eigVals,eigVects = linalg.eig(mat(covMat)) eigValInd = argsort(eigVals) # 从小到大对N个值排序 eigValInd = eigValInd[:-(topNfeat+1):-1] redEigVects = eigVects[:,eigValInd] # 将数据转换到新空间 lowDDataMat = meanRemoved * redEigVects reconMat = (lowDDataMat * redEigVects.T) + meanVals return lowDDataMat,reconMat'''将NaN替换成平均值'''def replaceNanWithMean(): datMat = loadDataSet('./data/Ch13/secom.data',' ') numFeat = shape(datMat)[1] for i in range(numFeat): meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[0],i]) # 将NaN置为平均值 datMat[nonzero(isnan(datMat[:,i].A))[0],i] = meanVal return datMatdataMat = replaceNanWithMean()meanVals = mean(dataMat,axis=0)meanRemoved = dataMat - meanValscovMat = cov(meanRemoved,rowvar=0)eigvals,eigVects = linalg.eig(mat(covMat))print(eigvals) 三、后记​ 这节通过PCA技术获取了数据压缩降维去噪的方法，在平时的监督和无监督学习中都可以用到，下一节要用SVD进行奇异值分解。 补充：​ PCA简单理解就是把一堆高维数据降到低维空间而不损失很多样本数据有用信息。比如，从二维将到一维中，其实就是将二维坐标系中的数据点垂直映射到一条方差最大的直线上，这里注意和线性回归求误差区分开，这里的映射是垂直，而线性回归的误差是求解竖直方向上的差值。 ​ 可想而知，映射肯定会存在映射误差的，我们如何计算映射误差呢？$$Loss = \\frac{1}{m}\\sum_{i=1}^{m}\\left | x^{i}-x_{appiox}^{i} \\right |^{2}$$其中m表示样本总数，x表示特征，x_appiox是映射后的特征和原始数据维度一样。","categories":[],"tags":[]},{"title":"FP-growth（Frequent Pattern growth）","slug":"FP-growth","date":"2019-04-16T16:00:00.000Z","updated":"2019-04-17T15:07:23.122Z","comments":true,"path":"2019/04/17/FP-growth/","link":"","permalink":"http://yoursite.com/2019/04/17/FP-growth/","excerpt":"","text":"一、算法理论1.FP-growth算法剧情介绍​ Apriori算法就可以获得频繁项集了，为什么还要用FP-growth呢？因为Apriori算法每次增加项获取频繁项集的时候都需要从新遍历一遍数据集这样效率还不高，FP-growth算法只需要遍历两边就可以获得所有频繁项集。这样在大数据集中会提高很多效率。 ​ 在生活中我们很多地方都用得到这个算法，比如搜索引擎，输入一个字，搜索引擎就会自动补全查询词项，当你输入”为什么“查询时，会出现一些推荐结果，这些词就是通过查看网络上经常在一起出现的词项来发现的，因此需要一种高效的发现频繁集的方法。 2.获取数据集 事务ID 事务中的元素项 001 r,z,h,j,p 002 z,y,x,w,v,u,t,s 003 z 004 r,x,n,o,s 005 y,r,x,z,q,t,p 006 y,z,x,e,q,s,t,m 3.公式原理​ 要提高找到频繁项集的效率就要用到FP树，我们通过构建FP-tree来压缩事务数据库中的信息，从而更加有效的发现频繁项集，FP-tree是一棵前缀树，按照支持度降序排列，支持度越高的频繁项离根节点越近，从而使得更多的频繁项可以共享前缀。 构建FP树​ 在FP树中，一个元素项可以出现多次，而且还会存储项集出现频率，每个项集以路径的方式存储，这里要对项集进行排序，按照支持度降序，这样的话我们就可以共享树的一部分。而且相似节点之间要进行连接。 ​ 比如：元素项z出现了5次，{r，z}出现了1次，{t,s,y,x,z}出现2次，{t,r,y,x,z}出现了1次，则自己单独还出现一次。这里说到{t,r,y,x,z}只出现过1次z，事务数据集中有{y,r,x,z,q,t,p}，q和p呢？这是因为q和p的支持度没有达到要求得频繁阈值被去掉了,这里设定的支持度为3，下标我们可以看到筛选过后并且通过频繁项排序的事务数据。 事务ID 事务中的元素项 过滤及重排序后的事务 001 r,z,h,j,p z,r 002 z,y,x,w,v,u,t,s z,x,y,s,t 003 z z 004 r,x,n,o,s x,s,r 005 y,r,x,z,q,t,p z,x,y,r,t 006 y,z,x,e,q,s,t,m z,x,y,s,t ​ 前边说过FP-growth算法要对数据集遍历两边，第一遍是对所有元素项出现次数统计，去掉那些不满足支持度的元素项。第二遍扫描只考虑那些频繁的元素，构建FP-tree，我们还需要一个头指针是通过项集的支持度排序后的字典列表，利用头指针指向树中的相同元素，然后相同元素进行连接。头指针还保存了树中每类节点的总个数。 ​ 构建FP树读入每个项集添加一条到路径中，其中头结点为NULL，如果路径不存在时新创建路径，存在时进行合并。 从FP树挖掘频繁项​ 获取到FP树后，我们就可以抽取频繁项集了，先从单个元素集合开始，然后逐步构建更大的集合，这里我们就要获得条件模式基，然后再利用条件模式基构建FP树，重复上述内容直到树包含一个元素项为止。 ​ 条件模式基是所查找元素为结尾的路径集合，每条路径是一条前缀路径，前缀路径是介于查找元素与根节点之间的所有内容。我们可以通过FP树得到每个频繁项的前缀路径： 频繁项 前缀路径 z {}5 r {x,s}1,{z,x,y}1,{z}1 x {z}3,{}1 y {z,x}3 s {z,x,y}2,{x}1 t {z,x,y,s}2,{z,x,y,r}1 ​ 我们可以通过前缀路径构建条件FP树，为了获取这些前缀路径，我们要对整个FP树进行遍历，直到获取到想要的频繁项为止。 ​ 然后创建条件FP树，我们以发现的条件模式基为输入数据，并通过相同的建树方式来构建这些树，然后递归发现频繁项，发现条件模式基，以发现另外的条件树。例如：频繁项t创建条件FP树，然后对{t,y},{t,x},…重复该过程，t的条件模式基有两个{z,x,y,s}:2和{z,x,y,r}:1，这里采用最小支持度为3，则会去掉s和r，因为构建出条件FP树时s与r都不满足最小支持度。在FP条件树中{t,r}和{t,s}不频繁，因此把r和s剔除。接下来对集合{t,z},{t,x},{ty}挖掘对应的条件树，这样会产生更加复杂的频繁项集，该过程重复进行，直到条件树中没有元素为止，就可以停止了。 ​ 通过上述方法就可以快速找到查找项的频繁项集。 4.获取频繁项集​ 构建FP树，然后构建条件FP树，直到条件树中为空为止，然后我们就可以快速查找相关的频繁项集了。 二、算法实践​ 这里我们可以实现的例子是从新闻网站点击流中挖掘。这里包含了近100万条记录，文件每一行为用户浏览过的新闻。有的用户只看过1篇报道，但是有的用户却看过很多报道，用户和报道都被编码成整数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121class treeNode: def __init__(self,nameValue,numOccur,parentNode): self.name = nameValue self.count = numOccur self.nodeLink = None self.parent = parentNode self.children = &#123;&#125; def inc(self,numOccur): self.count += numOccur def disp(self,ind = 1): print(\" \"*ind,self.name,\" \",self.count) for child in self.children.values(): child.disp(ind+1)def createTree(dataSet,minSup): headerTable = &#123;&#125; for trans in dataSet: for item in trans: headerTable[item] = headerTable.get(item,0) + dataSet[trans] # 移除不满足最小支持度的元素项 for k in list(headerTable.keys()): if headerTable[k] &lt; minSup: del(headerTable[k]) freqItemSet = set(headerTable.keys()) # 如果没有元素项满足要求，则退出 if len(freqItemSet) == 0: return None,None for k in headerTable: headerTable[k] = [headerTable[k],None] retTree = treeNode('Null Set',1,None) for tranSet,count in dataSet.items(): localD = &#123;&#125; # 根据全局频率对每个事物中的元素进行排序 for item in tranSet: if item in freqItemSet: localD[item] = headerTable[item][0] if len(localD) &gt; 0: orderedItems = [v[0] for v in sorted(localD.items(),key=lambda p:p[1],reverse=True)] # 使用排序后的频率项集对树进行填充 updateTree(orderedItems,retTree,headerTable,count) return retTree,headerTabledef updateTree(items,inTree,headerTable,count): if items[0] in inTree.children: inTree.children[items[0]].inc(count) else: inTree.children[items[0]] = treeNode(items[0],count,inTree) if headerTable[items[0]][1] == None: headerTable[items[0]][1] = inTree.children[items[0]] else: updateHeader(headerTable[items[0]][1],inTree.children[items[0]]) if len(items) &gt; 1: updateTree(items[1::],inTree.children[items[0]],headerTable,count)def updateHeader(nodeToTest,targetNode): while nodeToTest.nodeLink != None: nodeToTest = nodeToTest.nodeLink nodeToTest.nodeLink = targetNode'''简单数据集'''def loadSimpDat(): simpDat = [['r', 'z', 'h', 'j', 'p'], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'], ['z'], ['r', 'x', 'n', 'o', 's'], ['y', 'r', 'x', 'z', 'q', 't', 'p'], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']] return simpDatdef createInitSet(dataSet): retDict = &#123;&#125; for trans in dataSet: retDict[frozenset(trans)] = 1 return retDictdef ascendTree(leafNode,prefixPath): if leafNode.parent != None: prefixPath.append(leafNode.name) ascendTree(leafNode.parent,prefixPath)def findPrefixPath(basePat,treeNode): condPats = &#123;&#125; while treeNode != None: prefixPath = [] ascendTree(treeNode,prefixPath) if len(prefixPath) &gt; 1: condPats[frozenset(prefixPath[1:])] = treeNode.count treeNode = treeNode.nodeLink return condPatsdef mineTree(inTree,headerTable,minSup,preFix,freqItemSet): bigL = [v[0] for v in sorted(headerTable.items(),key=lambda p:p[1][0])] for basePat in bigL: newFreqSet = preFix.copy() newFreqSet.add(basePat) freqItemSet.append(newFreqSet) condPattBases = findPrefixPath(basePat,headerTable[basePat][1]) myCondTree,myHead = createTree(condPattBases,minSup) if myHead != None: print(\"conditional tree for:\",newFreqSet) myCondTree.disp(1) mineTree(myCondTree,myHead,minSup,newFreqSet,freqItemSet)parsedDat = [line.split() for line in open('./data/Ch12/kosarak.dat').readlines()]initSet = createInitSet(parsedDat)myFPtree,myHeaderTab = createTree(initSet,100000)myFreqList = []mineTree(myFPtree,myHeaderTab,100000,set([]),myFreqList)print(len(myFreqList))print(myFreqList) ​ 我们从中可以看到有多少报道曾经被十万人浏览过。 三、后记​ 我们通过FP-growth算法可以快速找到频繁项集，然后达到搜索引擎推荐效果等，接下来我们还可以对以往的数据集去噪降维，这样就会使得模型训练的更好。","categories":[],"tags":[]},{"title":"Apriori","slug":"Apriori","date":"2019-04-15T16:00:00.000Z","updated":"2019-04-16T14:25:54.781Z","comments":true,"path":"2019/04/16/Apriori/","link":"","permalink":"http://yoursite.com/2019/04/16/Apriori/","excerpt":"","text":"一、算法理论1.Apriori算法剧情介绍​ 当你去商店买东西的时候你会看到物品的各种摆放方式，购物之后优惠券，和用户忠诚度计划等，这些都是通过对大量数据的分析来制定的，通过顾客在商店购买商品的交易信息可以帮助商店了解用户的购买行为，从大规模数据集中寻找物品间的隐含关系，但是找出这些关系是一项巨大的计算工程，因此Apriori的诞生来解决上述问题。 2.获取相关数据​ 这里我们用商店的购买信息数据集： 交易号码 商品 0 豆奶，莴苣 1 莴苣，尿布，葡萄酒，甜菜 2 豆奶，尿布，葡萄酒，橙汁 3 莴苣，豆奶，尿布，葡萄酒 4 莴苣，豆奶，尿布，橙汁 3.公式原理​ 要讲述Apriori算法首先要介绍两个重要的概念：频繁项集、关联规则。 频繁项集​ 经常出现在一块的物品集合，比如{葡萄酒，尿布，豆奶}，什么才叫做频繁项集呢？顾名思义，经常出现的项集，我们可以设定一个支持度，当超过这个支持度的时候我们就认为这个项集为频繁项集。 ​ 支持度指的是数据集中包含该项集的记录所占的比例。比如{豆奶}的支持度为4/5，因为总共有5项，其中四项包含有豆奶。{豆奶，尿布}的支持度为3/5，因为有三项包含有{豆奶，尿布}。 关联规则​ 暗示两个物品之间可能存在着很强的关系，比如尿布—&gt;葡萄酒，意味着买尿布的人可能会买葡萄酒。关联规则又是如何区分物品之间有关联呢？我们这里定义了置信度，如果超过置信度代表有关联。 ​ 置信度指的是比如一条{尿布}—&gt;{葡萄酒}的关联规则可以定义为“支持度（{尿布，葡萄酒}）/支持度（{尿布}）”。比如，{尿布，葡萄酒}的支持度为3/5,尿布的支持度为4/5，所以尿布—&gt;葡萄酒的可信度为3/4=0.75。这意味着对于包含有尿布的所有记录中，这个规则对其中75%的记录都适用。 Apriori原理​ 当数据集小的时候我们可以很好找出频繁项集和关联规则，但是商品成千上万的时候这些就比较复杂，Apriori恰恰可以很好减少关联规则学习所需的计算量。 ​ 假如我们有商品0、1、2、3，我们的目标是找到经常在一起购买物品的集合。这里有四种商品我们要遍历所有的不同组合就需要15次，比如{0}，{1}…{3},{01},{02}…{23},{012},{013}…{123},{0123}可以生成这15种。假如有N种商品，我们就需要遍历2^N-1次，显然运算量很大。 ​ 但是我们发现，如果某个项集是频繁的，那么它的所有子集也是频繁的。比如：{0,1}是频繁的，那么{0}，{1}也一定是频繁的。我们可以得出，如果一个项集是非频繁的，那么对应的超项集也是非频繁的。 ​ 举个例子说明，假如项集{2,3}是非频繁的，那么{0,2,3}和{1,2,3}与{0,1,2,3}都是非频繁的，计算出{2,3}的支持度后就不需要计算后边的支持度了。这样就可以避免过多无用的计算。 ​ 接下来我们就可以确定关联规则了，比如有频繁项集{豆奶，莴苣}，我们就可以得到一条关联规则“豆奶—&gt;莴苣”。就意味着买豆奶就可能买莴苣。但是反过来就不一定了。 ​ 我们可以通过可信度来去掉小于可信度的关联规则，我们还发现，如果某个关联规则不满足最小可信度的要求，那么该规则的子集也不会满足最小可信度要求。比如0,1,2—&gt;3不满足最小可信度要求，那么任何{0,1,2}的子集都不会满足最小可信度要求。这样通过这个规则就可以减少关联规则计算的数目。 4.找出关联​ 通过Apriori算法我们可以找出数据之间的关联规则和频繁项集，这些数据给我们有很大的参考价值和意义，商品物品摆放等都有很好的作用。 二、算法实践​ 这里我们将实现一个案例，发现毒蘑菇的相似特征，我们可以通过算法找出毒蘑菇的一些公共特征，利用这些特征我们可以避免吃到那些有毒的蘑菇。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124'''创建简单数据集'''def loadDataSet(): return [[1,3,4],[2,3,5],[1,2,3,5],[2,5]]'''构建大小为1所有候选项的集合'''def createC1(dataSet): C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: C1.append([item]) C1.sort() return list(map(frozenset,C1))'''D:数据集Ck:包含候选集合的列表minSupport：感兴趣项的最小支持度'''def scanD(D,Ck,minSupport): ssCnt = &#123;&#125; for tid in D: for can in Ck: if can.issubset(tid): # 判断候选项中是否含数据集的各项 if not can in ssCnt: ssCnt[can] = 1 else: ssCnt[can] += 1 numItems = float(len(D)) retList = [] supportData = &#123;&#125; for key in ssCnt: support = ssCnt[key]/numItems if support &gt;= minSupport: retList.insert(0,key) supportData[key] = support return retList,supportData'''Lk:频繁项集列表k：项集元素个数输出：Ck'''def aprioriGen(Lk,k): retList = [] lenLk = len(Lk) for i in range(lenLk): # 前k-2个项相同时，将两个集合合并 for j in range(i+1,lenLk): L1 = list(Lk[i])[:k-2] L2 = list(Lk[j])[:k-2] L1.sort() L2.sort() if L1 == L2: retList.append(Lk[i]|Lk[j]) return retListdef apriori(dataSet,minSupport = 0.5): C1 = createC1(dataSet) D = list(map(set,dataSet)) L1,supportData = scanD(D,C1,minSupport) L = [L1] k = 2 while (len(L[k-2]) &gt; 0): Ck = aprioriGen(L[k-2],k) # 扫描数据集，从Ck得到Lk Lk,supK = scanD(D,Ck,minSupport) supportData.update(supK) L.append(Lk) k += 1 return L,supportDatadef calcConf(freqSet,H,supportData,brl,minConf=0.7): prunedH = [] for conseq in H: conf = supportData[freqSet]/supportData[freqSet-conseq] if conf &gt;= minConf: print(freqSet-conseq,'--&gt;',conseq,'conf:',conf) brl.append((freqSet-conseq,conseq,conf)) prunedH.append(conseq) return prunedHdef rulesFromConseq(freqSet,H,supportData,brl,minConf=0.7): m = len(H[0]) if len(freqSet) &gt; (m + 1): Hmp1 = aprioriGen(H,m+1) Hmp1 = calcConf(freqSet,Hmp1,supportData,brl,minConf) if len(Hmp1) &gt; 1: rulesFromConseq(freqSet,Hmp1,supportData,brl,minConf)'''L:频繁项集列表supportData:频繁项集支持数据的字典minConf:最小可信度阈值''' def generateRules(L,supportData,minConf=0.7): bigRuleList = [] for i in range(1,len(L)): for freqSet in L[i]: H1 = [frozenset([item]) for item in freqSet] if i &gt; 1: rulesFromConseq(freqSet,H1,supportData,bigRuleList,minConf) else: calcConf(freqSet,H1,supportData,bigRuleList,minConf) return bigRuleList mushDatSet = [line.split() for line in open('./data/Ch11/mushroom.dat').readlines()]L,suppData = apriori(mushDatSet,minSupport=0.3)for item in L[1]: if item.intersection('2'): print(item)for item in L[2]: if item.intersection('2'): print(item)for item in L[3]: if item.intersection('2'): print(item) 三、后记​ 通过这节的Apriori算法我们可以得出频繁项集与关联规则影响我们生活的方方面面。这里我们增加频繁项集的大小，Apriori算法就会重新扫描数据集，当数据集很大时会显著降低频繁项集发现速度，下一节FPgrowth算法只需要遍历两次，显著提高速度。","categories":[],"tags":[]},{"title":"KMeans","slug":"kMeans","date":"2019-04-14T16:00:00.000Z","updated":"2019-04-15T12:18:25.756Z","comments":true,"path":"2019/04/15/kMeans/","link":"","permalink":"http://yoursite.com/2019/04/15/kMeans/","excerpt":"","text":"一、算法理论1.KMeans算法剧情介绍​ KMeans是一种无监督学习算法，就是不知道这堆数据分别属于哪个类别，简单说就是没有标签数据，从中进行分堆的算法。 ​ 生活中很常见，物流配送站点的分配就是很好的聚类案例，假如我们要去个地方玩，想一下子游玩多个景点，但是现在我们有好多的景区，但是我们不知道距离多远，所以我们就可以用聚类算法来解决呀，可以把这些景区聚三堆，然后我们选择一堆去玩，找到合适的景区让我们一次性可以游玩多个景点。这就是聚类算法。 2.获取相关数据​ 我们可以获取到一些关于城市游玩地区的经纬度数据，如下： 序号 地区 经度 纬度 1 Dolphin -122.788346 45.486502 2 Hotties -122.781021 45.493150 … … … … 69 Stars Cabaret Bridgeport -122.765754 45.425788 70 Jiggles -122.753932 45.382682 3.公式原理​ 为了达到聚类效果，我们首先要选择簇的个数和判断距离标准，一般我们选择欧氏距离。 $$d=\\sqrt{\\left ( x1-x2 \\right )^{2}+\\left ( y1-y2 \\right ) ^{2}}$$其中x，y分别问样本的特征。一般我们还用距离作为误差的判断标准。 k-means聚类​ 这种聚类方式就是最简单的聚类，通过设定簇的个数，然后再设定距离公式，先随机初始化k个簇中心点，然后分别计算所有点距离哪个簇最近分配给哪个簇，然后持续更新簇中心点位置，最后簇中心位置不移动或者达到限制次数时就进行更新停止。 ​ 但是这种算法会存在一个问题，容易找到局部最优解，我们可以通过SSE(误差平方和)来评价算法的优劣，这样可以通过点距离中心的远近判断当前点簇的个数的分配是否合理。 二分k-means算法​ 这种算法和上一种算法计算方法一样，但是聚类划分的方式不一样，二分k-means算法先计算出一个簇时的中心，然后划分为两部分，然后再挑选这两部分划分后误差降低最多的簇划分为两部分，最终达到簇个数为止，这种算法很好解决了k-means算法陷入局部最优解的问题。 4.分类聚类​ 聚类算法就是把属于一个点簇的数据点给划分为一团，分为多个团，这里划分的团并不知道所代表的具体意思。 二、算法实践​ 我们通过以上数据集对于一个城市地方景区数据集进行聚类划分，选用几个中心城市点，可以做最少车次数游玩所有70个景点。经纬度采用经纬度方法计算距离。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136from numpy import *import matplotlibimport matplotlib.pyplot as plt'''读取文件数据转化为数据矩阵'''def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') fltLine = list(map(float,curLine)) dataMat.append(fltLine) return dataMat'''计算两个向量的欧氏距离'''def distEclud(vecA,vecB): return sqrt(sum(power(vecA - vecB,2)))'''随机生成k个质心'''def randCent(dataSet,k): n = shape(dataSet)[1] centroids = mat(zeros((k,n))) for j in range(n): minJ = min(dataSet[:,j]) rangeJ = float(max(dataSet[:,j]) - minJ) centroids[:,j] = minJ + rangeJ * random.rand(k,1) return centroids'''kMeans 聚类算法dataSet:数据集k：簇数目distMeas：计算距离createCent：创建质心'''def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2))) # 一列簇索引，一列误差 centroids = createCent(dataSet,k) clusterChanged = True while clusterChanged: clusterChanged = False for i in range(m): minDist = inf minIndex = -1 for j in range(k): distJI = distMeas(centroids[j,:],dataSet[i,:]) if distJI &lt; minDist: minDist = distJI minIndex = j if clusterAssment[i,0] != minIndex:# 发现存在点改变继续更新质心 clusterChanged = True clusterAssment[i,:] = minIndex,minDist**2 print(centroids) for cent in range(k): ptsInClust = dataSet[nonzero(clusterAssment[:,0].A == cent)[0]] centroids[cent,:] = mean(ptsInClust,axis=0) return centroids,clusterAssment'''二分K均值聚类算法'''def biKmeans(dataSet,k,distMeas=distEclud): m = shape(dataSet)[0] clusterAssment = mat(zeros((m,2))) # 列为分配的簇与误差 centroid0 = mean(dataSet,axis=0).tolist()[0] centList = [centroid0] for j in range(m): clusterAssment[j,1] = distMeas(mat(centroid0),dataSet[j,:])**2 while len(centList) &lt; k: lowestSSE = inf for i in range(len(centList)): ptsInCurrCluster = dataSet[nonzero(clusterAssment[:,0].A==i)[0],:] centroidMat,splitClustAss = kMeans(ptsInCurrCluster,2,distMeas) sseSplit = sum(splitClustAss[:,1]) sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:,0].A != i)[0],1]) print(\"sseSplit,and notSplit:\",sseSplit,sseNotSplit) if (sseSplit + sseNotSplit) &lt; lowestSSE: bestCentToSplit = i bestNewCents = centroidMat bestClustAss = splitClustAss.copy() lowestSSE = sseSplit + sseNotSplit # 更新簇的分配结果 bestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList) bestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit print(\"the bestCentToSplit is:\",bestCentToSplit) print(\"the len of bestClustAss is:\",len(bestClustAss)) centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0] centList.append(bestNewCents[1, :].tolist()[0]) clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:] = bestClustAss return mat(centList),clusterAssment'''球面距离计算'''def distSLC(vecA,vecB): a = sin(vecA[0,1]*pi/180) * sin(vecB[0,1]*pi/180) b = cos(vecA[0,1]*pi/180) * cos(vecB[0,1]*pi/180) * cos(pi * (vecB[0,0]-vecA[0,0]) /180) return arccos(a + b)*6371.0'''绘制图形'''def clusterClubs(numClust = 5): datList = [] for line in open('./data/Ch10/places.txt').readlines(): lineArr = line.split('\\t') datList.append([float(lineArr[4]),float(lineArr[3])]) datMat = mat(datList) myCentroids,clustAssing = biKmeans(datMat,numClust,distMeas=distSLC) fig = plt.figure() rect = [0.1,0.1,0.8,0.8] scatterMarkers = ['s','o','^','8','p','d','v','h','&gt;','&lt;'] axprops = dict(xticks=[],yticks=[]) ax0 = fig.add_axes(rect,label='ax0',**axprops) imgP = plt.imread('./data/Ch10/Portland.png') ax0.imshow(imgP) ax1 = fig.add_axes(rect,label='ax1',frameon=False) for i in range(numClust): ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A == i)[0],:] markerStyle = scatterMarkers[i % len(scatterMarkers)] ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0],ptsInCurrCluster[:,1].flatten().A[0],marker=markerStyle,s=90) ax1.scatter(myCentroids[:,0].flatten().A[0],myCentroids[:,1].flatten().A[0],marker='+',s=300) plt.show()# 测试clusterClubs(5) 三、后记​ 这种聚类方法是比较简单的一种，接下来说下Apriori算法。","categories":[],"tags":[]},{"title":"CART（Classification And Regression Trees）","slug":"CART","date":"2019-04-11T16:00:00.000Z","updated":"2019-04-13T07:44:26.741Z","comments":true,"path":"2019/04/12/CART/","link":"","permalink":"http://yoursite.com/2019/04/12/CART/","excerpt":"","text":"一、算法理论1.CART算法剧情介绍​ 当我们用一个线性模型去拟合所有样本的时候有时候效果并不好，因为数据的特征之间存在着十分复杂的关系，构建一个全局模型是就很困难，大多数我们遇到的问题都是非线性的，不可能用全局模型来拟合任何数据，因此我们可以将数据进行切分多份进行然后来进行建模。这里我们就可以结合树的划分优势与回归结合来拟合这些非线性的数据。 ​ 在之前的决策树算法中我们通过信息增益来进行划分，在CART算法中，我们采用二元划分来处理连续数据，对划分后的数据来计算平方误差差，找到最优的划分点。 2.获取相关数据​ 我们可以得到一组看电视时间与年龄的举例数据，如下： 看电视时间 年龄 3 12 4 18 2 26 5 47 2.5 36 3.5 29 4 21 3.公式原理​ 下面分别介绍回归树与模型树，其中这两种树最大的区别是：回归树的叶节点是单值，而模型树的叶节点是线性模型。 回归树​ CART树回归是二叉树，我们要遍历每个特征中的每个特征值，选择平方误差最小的作为最优划分点。然后以此特征的划分特征值分为左子树与右子树两部分。 ​ 当我们选择3时，{2}在左子树，{2.5，3，3.5，4，4，5}这样进行划分，然后计算出误差：$$Gain = \\sum_{i=1}^{2}\\sigma _{i}=0+\\sqrt{36^{2}+12^{2}+29^{2}+21^{2}+18^{2}+47^{2}-6\\ast27.67^{2} }=661.23$$​ 依次这样计算，选择方差最小的值为划分点进行划分。最后得到一个CART树。 树剪枝​ 但是当树节点过多的时候就会出现过拟合的问题，这里我们就要采用剪枝技术。这样可以提高模型的泛化能力。 1.预剪枝​ 我们可以通过提供一些参数限定切分后数据集中样本个数大小与减小误差大小是否符合当前你所规定的标准来判断是否进行分割或者进行预剪枝。通常我们设置一个tolN划分后最少节点个数与tolS误差减小的最小程度来判别。 2.后剪枝​ 但是我们也可以采用后剪枝的方法，后剪枝是先训练出一个足够复杂的决策树，然后用测试数据来测试将叶节点合并是否可以降低误差，如果降低了就合并。否则就不合并。 模型树​ 模型树其实和回归树类似，只不过在叶节点都是线性模型而不是一个常数（一组数据的均值），其中这里的误差计算方式不太一样，这里是先用线性回归拟合出来一条直线，然后计算出真实值与预测值之间的误差。 模型之间比较​ 不论是回归树、模型树、还是线性回归，我们可以通过相关系数来比较模型好坏。通过预测值与真实值之间的相关系数进行对比。通过计算预测值与真实值之间的相关系数，越接近1越好。 4.分析判断​ 总体来说CART算法就是通过对连续数据进行二分划分成两个数据集，然后计算和划分后的误差，我们要尽可能的拟合的较好，因此就需要找到最优的划分这就需要分别遍历每个特征中的每个特征值然后计算每种划分后的误差，选择最小的误差作为划分节点，以此类推，最终得到划分结果树。然后新来的数据可以通过该树得到最后的预测结果。 二、算法实践​ 下边实现了回归树与模型树的算法，通过骑自行车速度与智力测试数据集来对比两种模型的好坏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206from numpy import *from tkinter import *import matplotlibmatplotlib.use('TkAgg')from matplotlib.backends.backend_tkagg import FigureCanvasTkAggfrom matplotlib.figure import Figure'''加载文件数据'''def loadDataSet(fileName): dataMat = [] fr = open(fileName) for line in fr.readlines(): curLine = line.strip().split('\\t') # 将每行映射称为浮点数 fltLine = list(map(float,curLine)) dataMat.append(fltLine) return dataMat'''根据当前特征值切分数据dataSet:数据集合feature:带切分特征value:改特征的某个值'''def binSplitDataSet(dataSet,feature,value): mat0 = dataSet[nonzero(dataSet[:, feature] &gt; value)[0], :] mat1 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[0], :] return mat0,mat1'''创建叶节点'''def regLeaf(dataSet): return mean(dataSet[:,-1])'''误差计算'''def regErr(dataSet): return var(dataSet[:,-1]) * shape(dataSet)[0]'''选择最优划分特征ops:用于控制函数的停止时机'''def chooseBestSplit(dataSet,leafType=regLeaf,errType=regErr,ops=(1,4)): tolS = ops[0] # 容许的误差下降值 tolN = ops[1] # 切分的最少样本数 if len(set(dataSet[:,-1].T.tolist()[0])) == 1: return None,leafType(dataSet) m,n = shape(dataSet) S = errType(dataSet) bestS = inf bestIndex = 0 bestValue = 0 for featIndex in range(n-1): for splitVal in set((dataSet[:, featIndex].T.A.tolist())[0]): mat0,mat1 = binSplitDataSet(dataSet,featIndex,splitVal) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): continue newS = errType(mat0) + errType(mat1) if newS &lt; bestS: bestIndex = featIndex bestValue = splitVal bestS = newS if (S - bestS) &lt; tolS: return None,leafType(dataSet) mat0,mat1 = binSplitDataSet(dataSet,bestIndex,bestValue) if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): return None,leafType(dataSet) return bestIndex,bestValue'''dataSet:数据集leafType：建立叶节点的函数errType：误差计算函数ops：一个包含树结构所建所需其他参数的元组'''def createTree(dataSet,leafType=regLeaf,errType=regErr,ops=(1,4)): feat,val = chooseBestSplit(dataSet,leafType,errType,ops) if feat == None: return val retTree = &#123;&#125; retTree['spInd'] = feat retTree['spVal'] = val lSet,rSet = binSplitDataSet(dataSet,feat,val) retTree['left'] = createTree(lSet,leafType,errType,ops) retTree['right'] = createTree(rSet, leafType, errType, ops) return retTree'''判断当前节点是不是一颗树'''def isTree(obj): return (type(obj).__name__=='dict')'''获取树合并的节点均值'''def getMean(tree): if isTree(tree['right']): tree['right'] = getMean(tree['right']) if isTree(tree['left']): tree['left'] = getMean(tree['left']) return (tree['left'] + tree['right'])/2.0'''后剪枝函数tree:待剪枝的树testData：剪枝所需的测试数据'''def prune(tree,testData): if shape(testData)[0] == 0: return getMean(tree) if (isTree(tree['right']) or isTree(tree['left'])): lSet,rSet = binSplitDataSet(testData,tree['spInd'],tree['spVal']) if isTree(tree['left']): tree['left'] = prune(tree['left'],lSet) if isTree(tree['right']): tree['right'] = prune(tree['right'],rSet) if not isTree(tree['left']) and not isTree(tree['right']): lSet,rSet = binSplitDataSet(testData,tree['spInd'],tree['spVal']) errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) + sum(power(rSet[:,-1] - tree['right'],2)) treeMean = (tree['left'] + tree['right'])/2.0 errorMerge = sum(power(testData[:,-1] - treeMean,2)) if errorMerge &lt; errorNoMerge: print(\"合并\") return treeMean else: return tree else: return tree'''格式化数据集，求解w'''def linearSolve(dataSet): m,n = shape(dataSet) X = mat(ones((m,n))) Y = mat(ones((m,1))) X[:,1:n] = dataSet[:,0:n-1] Y = dataSet[:,-1] xTx = X.T*X if linalg.det(xTx) == 0.0: raise NameError(\"这个矩阵是奇异矩阵，不可以求逆,\\n 试着增加ops倍数\") ws = xTx.I * (X.T * Y) return ws,X,Y'''获取模型权重'''def modelLeaf(dataSet): ws,X,Y = linearSolve(dataSet) return ws'''计算线性模型误差'''def modelErr(dataSet): ws,X,Y = linearSolve(dataSet) yHat = X*ws return sum(power(Y-yHat,2))def regTreeEval(model,inDat): return float(model)def modelTreeEval(model,inDat): n = shape(inDat)[1] X = mat(ones((1,n+1))) X[:,1:n+1] = inDat return float(X*model)def treeForeCast(tree,inData,modelEval = regTreeEval): if not isTree(tree): return modelEval(tree,inData) if inData[tree['spInd']] &gt; tree['spVal']: if isTree(tree['left']): return treeForeCast(tree['left'],inData,modelEval) else: return modelEval(tree['left'],inData) else: if isTree(tree['right']): return treeForeCast(tree['right'],inData,modelEval) else: return modelEval(tree['right'],inData)def createForeCast(tree,testData,modelEval=regTreeEval): m = len(testData) yHat = mat(zeros((m,1))) for i in range(m): yHat[i,0] = treeForeCast(tree,mat(testData[i]),modelEval) return yHat# 骑自行车速度与智力测试trainMat = mat(loadDataSet('./data/Ch09/bikeSpeedVsIq_train.txt'))testMat = mat(loadDataSet('./data/Ch09/bikeSpeedVsIq_test.txt'))myTree = createTree(trainMat,ops=(1,20))yHat = createForeCast(myTree,testMat[:,0])rt = corrcoef(yHat,testMat[:,1],rowvar=0)[0,1]print(rt) 三、后记​ CART算法这里只介绍了连续型变量，其实现实生活中还会遇到离散型变量，我们也要划分成两份，然后计算基尼指数通过基尼指数来表示样本纯度，接下来介绍无监督学习。","categories":[],"tags":[]},{"title":"Python 进程和线程","slug":"python进程和线程","date":"2019-03-27T16:00:00.000Z","updated":"2019-03-30T13:54:39.749Z","comments":true,"path":"2019/03/28/python进程和线程/","link":"","permalink":"http://yoursite.com/2019/03/28/python进程和线程/","excerpt":"","text":"一、进程与线程简介​ 什么是进程呢？比如我们启动了qq软件，就是开启了一个任务，启动了word办公软件，这又是一个任务，简单举例就是平时我们边写文档，边听音乐，这就是多任务并行进行的，这里的任务就是进程。 ​ 现在我们的CPU处理器都是多核心的，可以执行多任务，在过去的时候CPU都是单核的，CPU上执行代码都是顺序执行的，我们也可以执行多任务，为何？这是因为操作系统的管控，它让每个任务分成了很多细小的片段，每个任务都在交替执行，任务1执行0.01秒，切换到任务2执行0.01秒，一直切换下去，由于CPU的执行速度太快了，让我们感觉就像多个任务同时执行一样。 ​ 其实真正的并行运行多任务只能在多核CPU上执行，但是任务的数目太多了，远远大于CPU核心数，因此操作系统会轮流调度多个任务在多个核心上执行。 ​ 对于操作系统来说呢，一个任务就是一个进程，我们打开了word就是开启了一个进程，但是进入word里边我们可以进行好多事情，打字，拼写检查，排版等多个子任务执行，这些子任务就是线程。由于每个进程都要至少干一件事情，因此一个进程至少有一个线程，像word这样的软件有多个线程来执行，多个线程可以同时执行，其中多线程执行方式和多进程执行方式一样，由操作系统在多个线程之间快速切换，让每个线程都在短暂的交替执行，让我们感觉就像同时执行一样。要实现真正的多线程要在多核CPU上才可以实现。 ​ 我们平时编写的算法代码都是单线程执行的，我们要执行多个任务的话可以采用以下方式： ​ 1.我们可以启动多个进程 ，每个进程只启动一个线程，多个进程可以执行多个任务。 ​ 2.我们还可以启动一个进程，在一个进程里启动多个线程，这样多线程可以执行多个任务。 ​ 3.我们还可以启动多个进程，每个进程里边再启动多个线程，这样的话比较复杂，容易出问题。 二，多进程Unix/Linux/Mac下创建进程​ 我们要实现多进程任务，我们就需要调用操作系统提供的调用函数fork()，其中fork()调用一次返回两次，这是因为操作系统把当前进程（父进程）复制一份进程（子进程），然后分别在父进程与子进程内返回。其中子进程永远返回为0，父进程返回子进程的ID，这样做的原因是，因为一个父进程可以创建很多个子进程，父进程要知道它所创建的所有子进程的ID，然而子进程调用getppid()就可以得到父进程的ID，我们可以通过getpid()获取子进程的ID。其中例子如下： 123456789import os# 这里我们是再Unix/Linux/Mac下才可以调用fork() print('进程(%s) 开启...' % os.getpid())pid = os.fork()if pid == 0: print('子进程是(%s)和父进程是(%s)' % (os.getpid(), os.getppid()))else: print('进程(%s)只创建了子进程(%s)' % (os.getpid(), pid)) Windows下创建进程​ 著名的开源项目Apache服务器就是用父进程来监听端口，当收到Http请求的时候，父进程就会创建子进程去处理请求。在Windows下不可以调用fork()，但是Python是跨平台语言，自然有可以创建进程的东西，这个就是multiprocessing模块就是多进程模块。multiprocessing中提供了Process类来代表一个进程对象如下例子： 123456789101112131415from multiprocessing import Processimport os# 子进程要执行的代码def run_proc(name): print('Run child process %s (%s)...' % (name, os.getpid()))if __name__=='__main__': print('父进程是%s' % os.getpid()) p = Process(target=run_proc, args=('test',)) print('子进程开启。。。') p.start() # 等待子进程完成任务 p.join() print('子进程执行完成') 其中创建子进程的时候要创建一个执行函数和函数的参数，这个函数里边就是子进程要执行的任务，用对象p调用start()方法开启子进程，用对象p调用join()方法来阻塞等待子进程完成。 进程池​ 假如我们需要创建大量的进程，我们此时就可以用进程池批量创建子进程，其中有Pool类来创建进程池。创建例子如下： 12345678910111213141516171819from multiprocessing import Poolimport os, time, randomdef long_time_task(name): print('运行任务%s(%s)...' % (name, os.getpid())) start = time.time() time.sleep(random.random()*3) end = time.time() print('任务%s运行%0.2f秒' % (name, (end - start)))if __name__=='__main__': print('父进程是%s' % os.getpid()) p = Pool(4) for i in range(5): p.apply_async(long_time_task, args=(i,)) print('等待子进程集完成...') p.close() p.join() print('所有子进程全部完成') 我们在这里用进程池创建了四个子进程，long_time_task为我们的进程函数，i为参数，子进程调用join方法等待所有子进程完成任务，调用join()前必须要先调用close()，调用close()后就不可以添加新的进程了。在代码中我们跑了5个进程，但是进程池的容量为4，因此最后一个进程要等待前面某一个子进程完成，进程池有空间了后再执行最后一个子进程。如果我们设置Pool(5)的话，就不需要等待了。一般Pool()中默认大小是CPU的核心数。 子进程​ 有时候进程并不是自身，而是外部进程，我们创建完成子进程后要控制输入与输出。例子如下： 12345import subprocessprint('$ nslookup www.python.org')r = subprocess.call(['nslookup', 'www.python.org'])print('停止代码:', r) 我们直接开启一个子进程子subprocess类来查询服务器信息。如果子进程需要输入，可以通过communicate()输入，例子如下： 1234567import subprocessprint('$ nslookup')p = subprocess.Popen(['nslookup'], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)output, err = p.communicate(b'set q=mx\\npython.org\\nexit\\n')print(output.decode('utf-8'))print('停止代码:', p.returncode) 上边意思是相当于执行nslookup命令后输入： 123set q=mxpython.orgexit 进程间通信​ 进程之间一定要通信的，操作系统提供了很多机制来控制通信。Python中的multiprocessing模块封装了底层机制，Queue，Pipes，等多种方式交换数据。接下来以Queue为例，创建两个子进程，一个往Queue里写入数据，一个从Queue里读取数据。例子如下： 12345678910111213141516171819202122232425262728293031from multiprocessing import Process, Queueimport os, time, random# 写数据进程def write(q): print('写的进程是: %s' % os.getpid()) for value in ['A', 'B', 'C']: print('放 %s 进入队列...' % value) q.put(value) time.sleep(random.random())# 读数据进程def read(q): print('读进程是: %s' % os.getpid()) while True: value = q.get(True) print('从队列里获取 %s ' % value)if __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 启动子进程pr，读取: pr.start() # 等待pw结束: pw.join() # pr进程里是死循环，无法等待其结束，只能强行终止: pr.terminate() ​ 在Linux和Unix下multiprocessing封装了fork()，我们不必关心底层细节，然而Windows中没有fork()函数，因此multiprocessing要模拟出fork()的效果，因此父进程所有的Python对象必须通过pickle序列化再传到子进程去。如果Window下调用multiprocessing失败，应该先考虑是不是pickle失败。 三、多线程​ 前面已经说过，多任务有三种实现方式，其中包括在一个进程下开启多个线程的方式，其中进程是由若干个线程组成的，一个进程至少有一个线程。线程是操作系统直接支持的执行单元，因此Python支持多线程。 多线程实例​ Python提供了两个模块，_thread，threading，__thread是低级模块，threading是高级模块，是对thread的封装，启动一个线程就是创建一个Thread实例，然后start()，就启动了一个线程，案例如下： 12345678910111213141516171819import time, threading# 新线程执行的代码:def loop(): print('thread %s is running...' % threading.current_thread().name) n = 0 while n &lt; 5: n = n + 1 # 打印子线程 print('thread %s &gt;&gt;&gt; %s' % (threading.current_thread().name, n)) time.sleep(1) print('thread %s ended.' % threading.current_thread().name) # 打印主线程print('thread %s is running...' % threading.current_thread().name)t = threading.Thread(target=loop, name='LoopThread')t.start()t.join()print('thread %s ended.' % threading.current_thread().name) 运行结果： 123456789thread MainThread is running...thread LoopThread is running...thread LoopThread &gt;&gt;&gt; 1thread LoopThread &gt;&gt;&gt; 2thread LoopThread &gt;&gt;&gt; 3thread LoopThread &gt;&gt;&gt; 4thread LoopThread &gt;&gt;&gt; 5thread LoopThread ended.thread MainThread ended 这里可以看到默认会启动一个线程就是主线程，主线程可以启动新的子线程，Python的threading模块有个current_thread()函数，可以永远返回当前线程实例。主线程的名称叫MainThread，子线程的名称自己可以指定，这里的LoopThread就是我们指定的线程名称，如果不起名字，Python就会自动命名，Thread-1，Thread-2等。 多线程Lock(锁)​ 多线程和多进程不同，在多进程中，同一个变量各自拷贝一份放在各自进程的存储空间里，各个进程之间操作互不影响。然而在多线程中，所有变量都由所有线程来共享，所以任何一个变量都可以被任何一个线程修改，因此，线程之间共享数据的危险在于多个线程同时修改一个变量，把内容改乱了。举一个例子来说明多线程如何改乱变量： 12345678910111213141516171819202122import time, threading# 假定这是你的银行存款:balance = 0def change_it(n): # 先存后取，结果应该为0: global balance balance = balance + n balance = balance - ndef run_thread(n): for i in range(100000): change_it(n)t1 = threading.Thread(target=run_thread, args=(5,))t2 = threading.Thread(target=run_thread, args=(8,))t1.start()t2.start()t1.join()t2.join()print(balance) 这里，我们定义了一个共享变量balance，初始值为0，开启了两个线程，先存后取，我们感觉结果应该是0，但是，线程的调度是操作系统控制的，当t1与t2交替执行时，只要循环次数足够多，balance就不一定为0了。这是因为高级语言的一条语句在CPU执行时是若干条语句，是执行一个简单计算，如：balance=balance+n，这个也是要分两部的，第一是：x = balance + n，存入临时变量中，然后第二是：将临时变量的值赋给balance。可以看成如下： 12x = balance + nbalance = x 由于x是局部变量，两个线程都各自修改各自的x，执行结果如下： 12345678910111213初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t2: x2 = balance - 8 # x2 = 8 - 8 = 0t2: balance = x2 # balance = 0结果 balance = 0 但是t1与t2是交替执行的，若操作系统按照下边执行: 123456789101112131415初始值 balance = 0t1: x1 = balance + 5 # x1 = 0 + 5 = 5t2: x2 = balance + 8 # x2 = 0 + 8 = 8t2: balance = x2 # balance = 8t1: balance = x1 # balance = 5t1: x1 = balance - 5 # x1 = 5 - 5 = 0t1: balance = x1 # balance = 0t2: x2 = balance - 8 # x2 = 0 - 8 = -8t2: balance = x2 # balance = -8结果 balance = -8 原因是因为修改balance需要多条语句，在执行这些语句的时候线程可能会会中断，从而导致多个线程把同一对象的内容改乱。 ​ 因此，为了保证数据在修改时不出错，就是一个线程修改时，其他线程无法修改。如果要保证上述的balance计算正确，就要给change_it()加上一把锁，当某个线程执行change_it()的时候，这个线程线程加了锁，其他线程无法同时执行change_it()，只能等待，直到锁释放后，获得该锁才能修改。因为锁只有一个，无论有多少线程，同一时刻只能有一个线程持有该锁，所以不会有冲突。创建锁用threading.Lock()来实现。 12345678910111213balance = 0lock = threading.Lock()def run_thread(n): for i in range(100000): # 先要获取锁: lock.acquire() try: # 放心地改吧: change_it(n) finally: # 改完了一定要释放锁: lock.release() 当多个线程执行 threading.Lock()的时候，只有一个线程会获得该锁，然后继续执行代码，其他线程就继续等待直到获得该锁为止。获得锁后使用完后要释放锁，如果不释放的话，其他线程就在那里苦苦等着获取该锁，就成了死线程，一般我们用try…finally来确保锁一定会被释放。我们获得锁好处就是只有一个线程执行某段代码，但是坏处也是有的，就是阻止了多线程的并发执行，包含锁的一段是单线程模式执行，效率会大大降低。由于可以存在多个锁，不同线程有不同的锁，并试图去获取对方持有的锁时，可能会造成死锁，导致多个线程全部挂起，既不能执行也不能结束，只能靠操作系统强制终止。 多核CPU​ 如果你有一个多核CPU，我们可以写一个死循环，可以查看CPU占用会达到100%，如果写两个死循环，在多核CPU中可以监控到200%CPU占用，就是占用了两个核心。如果要把n个CPU跑满，就必须要跑n个死循环，我们用Python写死循环: 12345678910import threading, multiprocessingdef loop(): x = 0 while True: x = x ^ 1for i in range(multiprocessing.cpu_count()): t = threading.Thread(target=loop) t.start() ​ 这里启动了与CPU核心数目相同的线程死循环个数，假如我们CPU是4核，那么就开启了四个线程，执行4个死循环，但是监控到CPU只占用了102%，就是使用可1个核，如果用其他语言C,C++，java来写相同死循环就可以跑满达到400%，为什么Python会这么差呢？因为Python线程虽然是真正的线程，但是解释器执行代码时，有一个GIL锁，任何Python线程执行之前，必须先获取GIL锁，然后执行100条字节码，解释器就是自动获取GIL锁，让别的线程有机会执行。这个GIL锁其实是把所有线程的执行代码都给上锁，所以多线程在Python中只能单线程执行，即使跑100个线程在100核CPU上，其实也是只用到了1个核。 ​ GIL是Python解释器遗留问题，通常我们用CPython这个解释器，要真正利用多核，除非写一个不带GIL的解释器。所以Python中可以使用多线程，但不能指望有效利用多核任务。若一定要用多核，只能通过C来扩展实现，不过这样就失去了Python简单易用的特点。不过Python不能通过多线程实现多核任务，但是可以用多进程实现多核任务，多个Python进程有多个GIL锁，互不影响。 四、ThreadLocal​ 在多线程环境中，每个线程都有自己的数据，一个线程使用自己的局部变量比全局变量要好，因为局部变量只能自己线程是可见的，不会影响其他线程，然而全局变量的修改必须加锁。但是局部变量也存在问题，在函数调用的时候传递很麻烦，举个例子： 12345678910111213def process_student(name): std = Student(name) # std是局部变量，但是每个函数都要用它，因此必须传进去： do_task_1(std) do_task_2(std)def do_task_1(std): do_subtask_1(std) do_subtask_2(std)def do_task_2(std): do_subtask_2(std) do_subtask_2(std) 这里每个函数调用都要传递下去，很麻烦，要用全局变量也不可以，因为每个线程处理的是不同的Student对象，不能共享。如果用一个dict存放所有Student对象，然后thread作为key获取线程对应的Student对象，这样会怎样？ 1234567891011121314151617global_dict = &#123;&#125;def std_thread(name): std = Student(name) # 把std放到全局变量global_dict中： global_dict[threading.current_thread()] = std do_task_1() do_task_2()def do_task_1(): # 不传入std，而是根据当前线程查找： std = global_dict[threading.current_thread()] ...def do_task_2(): # 任何函数都可以查找出当前线程的std变量： std = global_dict[threading.current_thread()] 这种方法理论可行，消除了对象std的传递，但是每个函数获取std对象的样子不美观，嗯。。。，我们可以用ThreadLocal呀，这可是个好东西，不用查找dict，ThreadLocal帮你自动做: 123456789101112131415161718192021import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('Alice',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('Bob',), name='Thread-B')t1.start()t2.start()t1.join()t2.join() 执行结果： 12Hello, Alice (in Thread-A)Hello, Bob (in Thread-B) 这里全局变量local_school就是ThreadLocal对象，每个Thread都可以对它读写student属性，互不影响。可以把local_school看成全局变量但是每个local_school.student都是线程的局部变量，还可以绑定其他变量，如：local_school.teacher。其中ThreadLocal最常用的地方就是每个线程绑定一个数据库链接HTTP请求，用户身份信息等， 五、进程VS线程进程线程优缺点​ 这里我们来说一下多进程与多线程的优缺点。 ​ 要实现多任务，我们通常会设计Master-Worker模式，Master负责分发任务，Worker负责执行任务，在多任务环境下通常有一个Master和多个Worker。多进程以Master-Worker模式实现，其中主进程是Master，子进程是Worker。多线程以Master-Worker模式实现，其中主线程是Master，子线程是Worker。 ​ 多进程最大优点就是稳定性高，因为一个进程崩溃了，不会影响主进程和其他子进程，如果主进程挂掉了，那么全部进程就挂掉了，著名开源项目Apache就是采用多进程来实现。多进程的缺点是创建进程的代价大，在Unix/Linux中，用fork()来创建还行，但是在Windows下创建进程开销会很大。操作系统能同时运行的进程数也是有限的，在内存和CPU的限制下，如果有几千个进程同时运行，操作系统调度就会有问题。 ​ 多线程的优点是通常比多进程要快一点，也不会快很多，而且，多线程有个致命的缺点是任何一个线程挂掉都可能造成整个进程的崩溃，因为所有线程都共享一个进程内存。在Windows下，如果一个线程代码出现问题，你会看到这样提示：改程序执行了非法操作，即将关闭。其实往往是某个线程出现了问题，但是操作系统会结束整个进程。 ​ 在Windows下，多线程要比多进程效率高，微软的IIS服务器采用的是多线程模式。由于多线程存在稳定性问题，所以IIS的稳定性就不如Apache，为了缓解这个问题，IIS又出现了多进程+多线程的混合模式。 线程切换​ 不管事多进程还是多线程，只要数目多，效率就会上不去，为什么呢？我们可以举一个例子，假如一个孩子在准备中考，每天晚上需要做语文，数学，英语，物理，化学这5门课程的作业，每项作业耗时1小时。如果先花1小时做完语文，然后再花1小时做完数学，依次下去一共花费5小时全部做完，这种方式为单任务模型，或批处理任务模型。假如打算切换到多任务去完成，可以先做1分钟语文，在做1分钟数学，一直切换交替执行下去，只要切换的速度足够快这种方式就和单核CPU执行多任务一样，那么人家可能看到你好牛逼，同时在写5门课程作业。但是切换是存在代价的，比如从语文切换到数学，我们要收拾桌子上的语文课本，水笔（称为保存现场），然后打开数学课本，找出圆规、直尺（这叫备新环境），才开始做数学作业。操作系统在切换进程和线程时候也是一样的，它先保存当前执行的现场环境（CPU寄存器状态，内存页等），然后把新任务的环境准备好（恢复上次寄存器状态，切换到内存页），然后才开始执行。虽然切换过程很快，但是也是需要消耗时间的，如果有几千个任务同时执行，操作系统可能就忙着切换任务去了，根本没有多少时间去执行任务，这种常见的情况就是硬盘狂响，点击窗口无反应，系统处于假死状态。因此，多任务一旦多个一个限度，就会消耗系统的所有资源，结果效率会急剧下降，所有任务都做不好。 计算密集型 vs IO密集型​ 是否采用多任务来执行的第二个考虑就是任务类型，一般分为计算密集型和IO密集型。 ​ 计算密集型的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频高清解码全靠CPU运算能力。计算密集行虽然可以用多任务完成，但是任务多，花在切换的时间就越多，CPU的执行效率就越低，因此，要想高效利用CPU，计算密集型同时运行的任务数量应当等于CPU核心数目。计算密集型主要消耗CPU资源，因此代码效率很重要，Python脚本执行效率低下完全不适合计算密集型任务。计算密集型任务最好用C语言去编写。 ​ IO密集型的特点是涉及到网络IO和磁盘IO的任务，都是IO密集型任务，这类任务的特点是CPU消耗很少，任务大部分时间都在等待IO操作完成（因为IO速度远远低于CPU执行速度），对于IO密集型任务，任务越多，CPU效率越高，但是也有一个限度。我们平时见到的很多都是IO密集型任务，比如WEB应用。IO密集型任务执行期间，99%的时间都花费在IO上，花在CPU的时间很少，因此用运行速度快的C去替换运行慢的Python也不会速度提升。对于IO密集型任务，最合适的开发语言就是开发效率高的语言，脚本语言是首选，C是比较差的。 异步IO​ 通常考虑到CPU和IO之间速度差异很大，一个任务执行过程中大部分时间花费在等待IO操作，单进程单线程模型会导致别的任务无法并行执行，因此我们要进行多进程或多线程模式并发执行。 ​ 现代操作系统对IO有很大的改善，最大特点是支持异步IO。如果充分利用操作系统异步IO的支持，就可以用单进程单线程来执行多任务，这种全新模型称为事件驱动模型，Nginx就是支持异步IO的web服务器，它在单核CPU上采用单进程单线程就可以支持高效的多任务。在多核CPU上，可以运行多个进程，充分利用多核CPU。由于系统总的进程数量有限，因此操作系统调度非常高效。 ​ 对于Python来说，单线程的异步编程编程模型称为协程，有了协程的支持，就可以基于时间驱动编写高效的多任务程序。 六、分布式进程​ 在线程和进程中，我们应该优先选择进程，因为进程更加稳定，而且进程可以分布到多台机器上，然而Thread最多只能分布到一台机器上的多个CPU上。 ​ Python的multiprocessing模块不仅支持多进程，其中managers子模块还支持把多进程分布到多台机器上。一台服务器作为调度者将任务分配到其他多个进程中，依靠网络通信。managers模块封装很好，使得我们不必了解网络通信底层细节，就很容易实现分布式多进程程序。举一个例子，如果我们已经有一个通过Queue通信的多进程程序在同一台机器上运行，现在，由于处理任务的进程任务繁重，希望把发送任务的进程和处理任务的进程分布到两台机器上，如何实现分布式？ ​ 原有的Queue可以继续使用，但是，通过managers模块把Queue通过网络暴露出去，就可以让其他机器的进程访问Queue了。我们可以先看服务进程，服务进程负责启动Queue，把Queue注册到网络上，然后往Queue里边写入任务。下边是Master进程： 1234567891011121314151617181920212223242526272829303132333435import random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('127.0.0.1', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 放几个任务进去:for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 从result队列读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') ​ 当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，必须通过manager.get_task_queue()来获得Queue接口添加。然后再另一台机器上启动任务进程（本机上也可以启动）。下边是Worker进程: 123456789101112131415161718192021222324252627282930313233import time, sys, queuefrom multiprocessing.managers import BaseManager# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行Master的机器:server_addr = '127.0.0.1'print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与Master设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n*n) time.sleep(1) result.put(r) except Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 任务进程要从网络连接到服务进程，所以要指定服务进程的ip地址。现在就可以达到分布式的效果。启动Master后输出： 1234567891011Put task 3411...Put task 1605...Put task 1398...Put task 4729...Put task 5300...Put task 7471...Put task 68...Put task 4219...Put task 339...Put task 7866...Try get results... 然后启动Worker后输出： 123456789101112Connect to server 127.0.0.1...run task 3411 * 3411...run task 1605 * 1605...run task 1398 * 1398...run task 4729 * 4729...run task 5300 * 5300...run task 7471 * 7471...run task 68 * 68...run task 4219 * 4219...run task 339 * 339...run task 7866 * 7866...worker exit. Worker进程结束后，在Master中会输出： 12345678910Result: 3411 * 3411 = 11634921Result: 1605 * 1605 = 2576025Result: 1398 * 1398 = 1954404Result: 4729 * 4729 = 22363441Result: 5300 * 5300 = 28090000Result: 7471 * 7471 = 55815841Result: 68 * 68 = 4624Result: 4219 * 4219 = 17799961Result: 339 * 339 = 114921Result: 7866 * 7866 = 61873956 这就是一个简单的分布式计算，我们可以改造成为多个Worker来处理任务，就可以把任务分发到几台或者几十台机器上。如果把计算n*n的队列换成发送邮件，那么就实现了队列的异步发送。 ​ 其中Queue之所以能够通过网络访问是因为通过QueueManager实现的。因为QueueManager不止管理一个Queue，所以要给每个调用接口起一个名字，比如get_task_queue。authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。如果Master和Worker的Queue不一致，那么就会连接不上。","categories":[],"tags":[]},{"title":"线性回归（Line Regression）","slug":"Regression","date":"2019-03-26T16:00:00.000Z","updated":"2019-03-28T13:48:26.433Z","comments":true,"path":"2019/03/27/Regression/","link":"","permalink":"http://yoursite.com/2019/03/27/Regression/","excerpt":"","text":"一、算法理论1.线性回归算法剧情分析​ 线性回归在实际生活中很常见，比如股市预测，某商品价格预测等，其实还有许多有趣的事情，预测名人的离婚率。接下来同样举一个例子来说明。 ​ 假如我们要通过历史数据预测鲍鱼的年龄，参考的历史数据有好多特征，这些特征有不同的权重，最后构成了这只鲍鱼生存的年龄，我们通过这些特征构成多元一次方程来拟合数据，然后再来新数据的时候我们就可以预知这只鲍鱼能活多久了，其实很简单，线性回归。 2.获取数据​ 以下是历史鲍鱼存活年龄以及生理特征。 序号 x1 … x9 年龄 1 0.455 … 0.15 15 2 0.35 … 0.07 7 … … … … … 4176 0.625 … 0.296 10 4177 0.71 … 0.495 12 3.公式原理​ 对于一个含有多个特征的数据我们一般会列出线性回归的公式：$$\\hat{y} = x_{0}+w_{1}x_{1}+…+w_{n}x_{n}$$这里n为特征的个数，x0为偏差常数项，w是每个特征所占有的比重也就是权重，x为样本特征值，y尖为预测的值。也可以表示为：$$\\hat{y}=x_{i}^{T}w\\, \\, \\, \\, \\, \\, i=1,2,…,n$$这里x为一个样本特征，w为样本权重，我们可以通过预测值与真实值的差值计算训练时的误差，但是有时后误差有正有负会抵消，因此我们采用平方误差来表示：$$\\sum_{i=1}^{n}(y_{i}-x_{i}^{T}w)^{2}$$ 用矩阵表示的话：$$(y-Xw)^{T}(y-Xw)$$对w求导等于0即可得到最小值的点，得：$$\\hat{w}=(X^{T}X)^{-1}X^{T}y$$w帽表示求出来的w的近似最优解。求出最优w后就可以把新样本数据带入求解预测值。 局部加权线性回归​ 我们采用一般的线性回归或许的到的最优拟合直线误差还是很大，造成欠拟合问题，因此我们对样本待预测点的附近的数据点添加一定的权重，即可以得出此时的误差表示为：$$\\sum_{i=1}^{n}a_{i}(y_{i}-x_{i}^{T}w)^{2}$$这里ai为权重，我们求导等于0得：$$\\hat{w}=(X^{T}AX)^{-1}X^{T}Ay$$这里A为ai的矩阵形式。通常情况下ai取得为：$$a_{i}=exp\\left ( \\frac{|x^{(i)}-x|}{-2k^{2}} \\right )$$在高斯图像中我们可以得知，xi与x越近则ai越趋近于1，xi与x越远，ai越趋近于0，其中xi为新预测样本数据，也就是说，距离待预测样本越近权重越大。这样构造出来的权重矩阵A是对角矩阵，可以见得，当k取得越小的时候，高斯图像的山峰越陡峭，那么就有好多距离预测样本远的数据权重几乎为0，只要很少一部分样本用于训练回归模型。其实可以绘制预测的数据与真实数据对比，k取得越小，对训练样本就拟合的越好，那么就可能陷入过拟合的问题，因此要选择合适的k值。 岭回归​ 假如，在上述求解w的时候要求逆的矩阵是奇异矩阵，那么我们就计算不了了，因此，我们这里采用在矩阵上加一个λI来使得矩阵可以求逆，这里I是特征数目大小的单位矩阵，λ为设定的值，那么求解回归系数的公式就变为：$$\\hat{w}=(X^{T}X+\\lambda I)^{-1}X^{T}y$$岭回归一般用于特征数目多于样本数目，这时候容易出现过拟合，这里λ限制了所有w之和，通过引入惩罚项，减少不必要的参数。 lasso​ 在增加约束的时候，普通的最小二乘法回归得到与岭回归一样的式子：$$\\sum_{i=1}^{n}w_{i}^{2}\\leq \\lambda$$这样，在岭回归中就可以避免出现正负的权重相消，但是lasso采用的是：$$\\sum_{i=1}^{n}|w_{i}|\\leq \\lambda$$来对回归系数做了限定。这样做的好处是当λ很小的时候，一些很小的权重就会缩减到0。 前向逐步回归​ 这个算法是基于贪心的原则，每一步都尽可能的减小误差，权重初始化都为1，然后每一步决策权重都会增加和减小一个很小的值。 4.分析预测​ 这一节的回归模型其实就是根据已经有的训练数据列出特征公式，然后我们通过尽可能的缩小误差去求解最优的权重w，然后通过w我们就可以预测出最新的值了。 二、算法实践​ 这里实现了预测鲍鱼年龄的案例，我们对数据做了标准化，这样每个特征的数据权重就可以等价。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152from numpy import *import matplotlib.pyplot as pltfrom time import sleepimport jsonimport urllib3'''读取数据'''def loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t'))-1 dataMat = [] labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = [] curLine = line.strip().split('\\t') for i in range(numFeat): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMat'''计算最佳拟合直线'''def standRegres(xArr,yArr): xMat = mat(xArr) yMat = mat(yArr).T xTx = xMat.T*xMat if linalg.det(xTx) == 0.0: # 利用线性代数库计算行列式 print(\"这个矩阵是奇异矩阵，不可以求逆\") return ws = xTx.I * (xMat.T*yMat) return ws'''局部加权线性回归'''def lwlr(testPoint,xArr,yArr,k=1.0): xMat = mat(xArr) yMat = mat(yArr).T m = shape(xMat)[0] weights = mat(eye((m))) for j in range(m): diffMat = testPoint - xMat[j,:] weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2)) xTx = xMat.T * (weights * xMat) if linalg.det(xTx) == 0.0: print(\"这个矩阵是奇异矩阵，不能取逆\") return ws = xTx.I * (xMat.T * (weights * yMat)) return testPoint * wsdef lwlrTest(testArr,xArr,yArr,k=1.0): m = shape(testArr)[0] yHat = zeros(m) for i in range(m): yHat[i] = lwlr(testArr[i],xArr,yArr,k) return yHat'''计算误差'''def rssError(yArr,yHatArr): return ((yArr-yHatArr)**2).sum()'''计算回归系数(岭回归)'''def ridgeRegres(xMat,yMat,lam=0.2): xTx = xMat.T*xMat denom = xTx + eye(shape(xMat)[1])*lam if linalg.det(denom) == 0.0: print(\"这个矩阵是奇异矩阵，不能取逆\") return ws = denom.I * (xMat.T*yMat) return ws'''用于在一组lambda上测试结果'''def ridgeTest(xArr,yArr): xMat = mat(xArr) yMat = mat(yArr).T yMean = mean(yMat,0) # 数据标准化 yMat = yMat - yMean xMeans = mean(xMat,0) xVar = var(xMat,0) xMat = (xMat - xMeans)/xVar numTestPts = 30 wMat = zeros((numTestPts,shape(xMat)[1])) for i in range(numTestPts): ws = ridgeRegres(xMat,yMat,exp(i-10)) wMat[i,:] = ws.T return wMat'''数据标准化'''def regularize(xMat): inMat = xMat.copy() inMeans = mean(inMat,0) inVar = var(inMat,0) inMat = (inMat - inMeans)/inVar return inMat'''逐步线性回归xArr：样本数据xArr：预测变量eps:迭代需要调整的步长numIt:迭代次数'''def stageWise(xArr,yArr,eps=0.01,numIt=100): xMat = mat(xArr) yMat = mat(yArr).T yMean = mean(yMat,0) yMat = yMat - yMean xMat = regularize(xMat) m,n = shape(xMat) returnMat = zeros((numIt,n)) ws = zeros((n,1)) wsTest = ws.copy() wsMax = ws.copy() for i in range(numIt): print(ws.T) lowestError = inf for j in range(n): for sign in [-1,1]: wsTest = ws.copy() wsTest[j] += eps*sign yTest = xMat*wsTest rssE = rssError(yMat.A,yTest.A) if rssE &lt; lowestError: lowestError = rssE wsMax = wsTest ws = wsMax.copy() returnMat[i,:] = ws.T return returnMat abX,abY = loadDataSet('./data/Ch08/abalone.txt')# 预测鲍鱼的年龄yHat01 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],0.1)yHat1 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],1)yHat10 = lwlrTest(abX[0:99],abX[0:99],abY[0:99],10)print(rssError(abY[0:99],yHat01.T))print(rssError(abY[0:99],yHat1.T))print(rssError(abY[0:99],yHat10.T)) 三、后记​ 这节主要阐述了回归的线性回归问题，其中遇到了过拟合问题，通过添加正则化项来解决。 权衡偏差与方差偏差​ 一般模型复杂度低的时候容易出现高的偏差，在上边介绍的系数缩减的方案就是增加偏差，同时降低了模型的预测误差。 方差​ 例如，我们对上边鲍鱼年龄预测的例子，我们先随机抽取100例求出最优权重系数，然后再随机抽取100例求出最优权重系数，最后比较这两组权重系数的差异就是方差大小的反映。 ​ 一般我们采取的是偏差方差折中的方式来求解权重系数。","categories":[],"tags":[]},{"title":"AdaBoost（adaptive boosting）","slug":"AdaBoost","date":"2019-03-25T16:00:00.000Z","updated":"2019-03-27T02:59:04.081Z","comments":true,"path":"2019/03/26/AdaBoost/","link":"","permalink":"http://yoursite.com/2019/03/26/AdaBoost/","excerpt":"","text":"一、算法理论1.AdaBoost算法剧情介绍​ 想一下，当我们在做一件重要事情的时候我们就会参考周围朋友们和家人的意见，而不是一个人去做决断。其实这个算法的原理也是这样的，通过对多个弱分类器的集成，观察最后多个弱分类器的分类结果，按照投票法，类别数目最多的就是该样本的类别，输出最后的分类结果。举一个例子来说这个算法更为合适。 ​ 假如你要结婚了，人家给你介绍了两个对象a和b，去约会后，回到家中，亲戚朋友和家人都会问，女孩漂亮吗？性格怎么样？你喜欢吗？等等这些问题，你回答说，a很漂亮，性格有小脾气，很喜欢。b长相一般，脾气很好，不喜欢。结果这其中每个大姑，三姨，爸爸妈妈都给出了各自的意见，其中肯定爸爸妈妈的意见更重要了，所以听取他们意见之后最后选择一个对象作为伴侣。其中这里边大姑，三姨，爸爸妈妈就是各个弱分类器，因为每个人的亲近程度不同，导致不同分类器的分配权重不同，最后我们根据计算出最后的值进行比较选择哪个。 2.获取相关数据​ 这里的数据是逻辑回归算法那节对马获得氙气病后是否死亡的数据集。 序号 x1 x2 … x21 类别 1 2.0 1.0 … 0.0 -1 2 1.0 1.0 … 2.0 -1 … … … … … … 298 1.0 1.0 … 3.4 1 299 1.0 1.0 … 1.0 -1 3.公式原理bagging​ 简单来说就是对原始数据集选择S次得到S个新的样本集，且这新的S个样本集与原来样本集的数目相同且抽样是放回的抽样，会出现重复的情况。然后我们会用弱分类器对这S个数据集进行训练得到S个分类器，然后新样本来的时候通过这S个分类器会得到S个分类结果，采用投票法，选择相同类别数目最多的作为最终类别。 boosting​ boosting与bagging差不多，但是boosting更关注于被分类器分类错误的那些数据来训练新的分类器。boosting的分类结果是不同分类器的加权结果，其中在bagging权重都是相同的，但是这里不同，每个权重代表了分类器上一轮迭代中的成功度。其中最流行的boosting算法是AdaBoost算法，接下来用树桩决策树作为弱分类器来构建元算法。 基于错误提升分类器性能​ 可以通过多个弱分类器构建出来一个强分类器，其中每个数据样本要设置一个权重，得到权重向量D，开始这些权重都是一样的值。首先用弱分类器训练样本得出分类器，计算分类器错误率。然后在同一个数据集上再次训练分类器，第二次训练的时候就会更新每个样本的权重，其中第一次分类正确的样本权重会减小，分类错误的样本权重会增加。最后为每次训练的弱分类器分配一个权重alpha，其中这些权重alpha基于每个弱分类器的错误率ε进行计算。$$\\varepsilon = \\frac{分类错误样本数}{所有样本数}$$alpha计算公式：$$\\alpha =\\frac{1}{2}\\ln \\left ( \\frac{1-\\varepsilon }{\\varepsilon } \\right )$$如果对样本分类正确，对权重向量D进行更新：$$D^{(t+1)}{t}=\\frac{D^{(t)}{t}e^{-\\alpha }}{Sum(D)}$$如果对样本分类错误，对权重向量D进行更新：$$D^{(t+1)}{t}=\\frac{D^{(t)}{t}e^{\\alpha }}{Sum(D)}$$计算出D后，又进入下一轮迭代。直到分类器的错误率为0或者达到用户指定值。我们可以通过多个分类器来计算每个类别的预测值。$$pred = \\sum_{i=1}^{n}\\alpha {i}pred{i}$$pred表示本次分类器预测值，最后计算出结果后通过sign函数得出分类结果，可以计算错误率。 4.分析判断​ Adaboost算法就是通过分类器每次分类对样本的错误率来分配给样本不同的权重值，然后每个分类器会获得一个权重alpha，最后达到指定停止值时，通过权重和alpha权重求和，然后通过sign函数得出分类结果。 二、算法实践​ 对于预测马是否死亡的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import numpy as npimport matplotlib.pyplot as plt'''通过阈值比较对数据进行分类'''def stumpClassify(dataMatrix,dimen,threshVal,threshIneq): retArray = np.ones((np.shape(dataMatrix)[0],1)) if threshIneq == 'lt': retArray[dataMatrix[:,dimen] &lt;= threshVal] = -1.0 else: retArray[dataMatrix[:,dimen] &gt; threshVal] = -1.0 return retArray'''找到最佳的单层决策树'''def buildStump(dataArr,classLabels,D): dataMatrix = np.mat(dataArr) labelMat = np.mat(classLabels).T m,n = np.shape(dataMatrix) numSteps = 10.0 bestStump = &#123;&#125; bestClasEst = np.mat(np.zeros((m,1))) minError = np.inf for i in range(n): rangeMin = dataMatrix[:,i].min() rangeMax = dataMatrix[:,i].max() stepSize = (rangeMax-rangeMin)/numSteps for j in range(-1,int(numSteps)+1): for inequal in ['lt','gt']: threshVal = (rangeMin + float(j) * stepSize) predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal) errArr = np.mat(np.ones((m,1))) errArr[predictedVals == labelMat] = 0 # 计算加权错误率 weightedError = D.T*errArr print(\"split:dim %d,thresh %.2f,thresh ineqal:%s,the weighted error is %.3f\" % (i,threshVal,inequal,weightedError)) if weightedError &lt; minError: minError = weightedError bestClasEst = predictedVals.copy() bestStump['dim'] = i bestStump['thresh'] = threshVal bestStump['ineq'] = inequal return bestStump,minError,bestClasEst'''基于单层决策树的AdaBoost训练dataArr:数据集classLabels:类别标签numIt：迭代次数'''def adaBoostTrainDS(dataArr,classLabels,numIt=40): weakClassArr = [] m = np.shape(dataArr)[0] D = np.mat(np.ones((m,1))/m) aggClassEst = np.mat(np.zeros((m,1))) for i in range(numIt): bestStump,error,classEst = buildStump(dataArr,classLabels,D) print(\"D:\",D.T) alpha = float(0.5*np.log((1.0-error)/max(error,1e-16))) bestStump['alpha'] = alpha weakClassArr.append(bestStump) print(\"classEst:\",classEst.T) # 为下一次迭代计算D expon = np.multiply(-1*alpha*np.mat(classLabels).T,classEst) D = np.multiply(D,np.exp(expon)) D = D/D.sum() aggClassEst += alpha*classEst print(\"aggClassEst:\",aggClassEst.T) # 错误率累加计算 aggErrors = np.multiply(np.sign(aggClassEst) != np.mat(classLabels).T,np.ones((m,1))) errorRate = aggErrors.sum()/m print(\"total error:\",errorRate,\"\\n\") if errorRate == 0.0: break return weakClassArr,aggClassEst'''AdaBoost分类函数'''def adaClassify(datToClass,classifierArr): dataMatrix = np.mat(datToClass) m = np.shape(dataMatrix)[0] aggClassEst = np.mat(np.zeros((m,1))) for i in range(len(classifierArr)): classEst = stumpClassify(dataMatrix,classifierArr[i]['dim'],classifierArr[i]['thresh'],classifierArr[i]['ineq']) aggClassEst += classifierArr[i]['alpha']*classEst print(aggClassEst) return np.sign(aggClassEst)'''自适应数据加载'''def loadDataSet(fileName): numFeat = len(open(fileName).readline().split('\\t')) dataMat = [] labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = [] curLine = line.strip().split('\\t') for i in range(numFeat-1): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) return dataMat,labelMat'''ROC曲线的绘制及AUC计算predStrengths:分类器预测强度classLabels：分类标签'''def plotROC(predStrengths,classLabels): cur = (1.0,1.0) ySum = 0.0 numPosClas = sum(np.array(classLabels)==1.0) yStep = 1/float(numPosClas) xStep = 1/float(len(classLabels)-numPosClas) # 获取排好序的索引 sortedIndicies = predStrengths.argsort() fig = plt.figure() fig.clf() ax = plt.subplot(111) for index in sortedIndicies.tolist()[0]: if classLabels[index] == 1.0: delX = 0 delY = yStep else: delX = xStep delY = 0 ySum += cur[1] ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY],c='b') cur = (cur[0] - delX,cur[1]-delY) ax.plot([0,1],[0,1],'b--') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('ROC curve for AdaBoost Horse Colic Detection System') ax.axis([0,1,0,1]) plt.show() print(\"the Area Under the Curve is:\",ySum*xStep)datArr,labelArr = loadDataSet('./data/Ch07/horseColicTraining2.txt')classifierArr,aggClassEst = adaBoostTrainDS(datArr,labelArr,10)plotROC(aggClassEst.T,labelArr) 三、后记​ 通过案例分析了解集成学习中的adaboost算法，使用这种方式集成是现在非常流行的方式。 非均衡分类问题​ 比如一个垃圾邮件分类问题，如何去过滤垃圾邮件呢？其收件箱可以出现垃圾邮件，但是垃圾文件夹不会出现合法的邮件，是否这样就会符合呢？比如，癌症检测问题，可以把正常人检测为患有癌症去复查，但是不允许出现患有癌症被分类为正常人，这是关乎生命的问题。大多数情况下不同类别分类代价是不相等的，接下来说下非均衡分类问题的分类器性能度量方法。 ​ 一般情况下我们都是看下分错样本占总样本的比例来描述错误率，没有详细描述对于哪类分类错误的概率，这里我们换一种方式度量，这里有二维的混淆矩阵： 预测结果正例（+1） 预测结果反例（-1） 真实结果正例（+1） 真正例（TP） 伪反例（FN） 真实结果反例（-1） 伪正例（FP） 真反例（TN） 其中将一个真正的正例预测为正例，真正例就加一，如果一个真正的反例预测为反例，真反例就加一。 准确率​ 在分类当中，当某个类别的重要性高于其他类别的时候，比如上边的癌症预测，通过正确率就可以更好的描述预测正例样本中，真正例所占有的比例，可以表示为：$$准确率（precision）=\\frac{TP}{TP+FP}$$ 召回率​ 其中召回率给出了，预测正例占真正结果为正例的比例，可以表示为：$$召回率（recall）=\\frac{TP}{TP+TN}$$​ 我们可以构建出高的准确率或召回率，但是两者很难兼得。如果任何样本都预测为正例的话，召回率很高呀等于一，准确率就不高等于正例占总样本的比例。因此构建两者都高的分类器不容易。 ROC曲线​ ROC曲线也是衡量分类中的非均衡分类性的工具，在二战期间一个电气工程师构建雷达系统时候使用过。在ROC曲线里边有一个实线和一个虚线，其中横轴是假阳率（FP/(FP+TN)），纵轴是真阳率（TP/(TP+FN)）,ROC曲线给出的当阈值变化时，假阳率和真阳率的变化情况，其中左下角也就是两者都为0的时候，是将所有样例判断为反例的情况，而右上角就是两者都为1的时候，是将所有样本都判断为正例的情况，虚线是随机猜测的结果。 ​ 一般情况下，好的分类器都位于左上角，因为此时有较好的真阳率和低的假阳率，例如在垃圾邮件过滤中，过滤掉了所有的垃圾邮件，没有把合法的邮件过滤掉。对ROC曲线比较的一个性能指标就是AUC，指的是曲线下的面积，AUC给出的是分类器平均性能值，其中一个完美的分类器AUC=1,随机猜测AUC=0.5。 基于代价函数选择最佳分类器​ 在之前找到当前弱分类器最佳是通过过划分不同的阈值，比较错误率。我们还可以利用代价矩阵的方式来处理非均衡分类，由于分类错误的代价不一样，所以获取的收益也就不一样，最后选择付出代价最小的分类器。代价矩阵如下： 预测结果（+1） 预测结果（-1） 真实结果（+1） -5 1 真实结果（-1） 50 0 分类代价计算：$$TP\\times (-5)+FN\\times 1+FP\\times 50+TN\\times 0$$ 处理非均衡数据抽样方法​ 这里主要是对分类器的分类数据进行改造。 1.欠抽样​ 欠抽样就是删除样例 2.过抽样​ 过抽样是复制样例 ​ 举一个例子，信用卡欺诈问题，正例样本比较罕见，我们需要保留更多的正例信息，我们就保留了正例的所有信息，然而反例正我们进行欠抽样，对样本进行删除，可以随机也可以以某个预订方式。这种方法存在一个问题就是，如何剔除样本，有可能剔除样本中包含了有价值的信息（本应该不剔除的，但是剔除了），那么我们可以选择离决策边界较远的进行剔除。假如有50例信用卡欺诈的样本和5000个合法样例，我们就要对合法样例进行欠抽样，去除4950个，这些样本中可能包含很多有价值的信息，但是去除了，我们可以采用启发式方法，就是对正例过抽样，对反例样本进行欠抽样这种混合的方式。","categories":[],"tags":[]},{"title":"python爬取微信公众号内容","slug":"爬取微信公众号数据","date":"2019-03-24T16:00:00.000Z","updated":"2019-03-25T08:00:56.474Z","comments":true,"path":"2019/03/25/爬取微信公众号数据/","link":"","permalink":"http://yoursite.com/2019/03/25/爬取微信公众号数据/","excerpt":"","text":"一、背景​ 为什么要做这个呢，一个同学需要一个公众号的数据进行数据分析，要有的信息有：发帖日期，发帖标题，点赞数（好看），浏览量，之前做过简单爬虫，爬取过MIT网站的EEG数据集，就尝试做了下这个数据爬虫。刚开始想到去能够获取微信公众号内容的搜狗公众号接口，微信公众平台接口中获取，就选择了微信公众平台的发布信息的接口来进行爬取。 二、准备1.微信公众号的用户名，密码​ 首先打开微信公众平台，登录，扫码进入平台。点击”素材管理”—&gt;”新建素材”，然后点击书写内容的上边工具栏的”超链接”标识，按F12点击下边的network项，看有没有”appmsg?=media/appmsg_edit_v2&amp;….“,如果没有按”CTRL+R“刷新试试，查看这个的Headers,找到cookies和User-Agent，保存在一个地方，一会儿要用。然后选择”查找文章“，输入要爬取公众号的名称，点击选中，然后按F12，找到”appmsg?token=182255…“等，点击headers,找到token与fakeid，保存下来，一会儿要用。其实用这些信息就可以获取到公众号的内容了，但是我们这里打开的网页只有文章信息，没有评论与点赞数（好看）与浏览量。 2.Fiddler抓包工具​ 为了获取浏览量与点赞数，我们想到在微信PC端是不是有这些数据，因此我们可以用Fiddler来抓取接口数据，打开微信客户端，打开Fiddler，查询出自己想要爬取的公众号，进入，点击“查看历史消息”，然后选择一篇文章，然后查看Fiddler中请求成功的http消息，找到HOST为“mp.weixin.qq.com”，URL为“/mp/getappmsgext?f=….”的url，点击选中，看右侧上部，选择“WebForms”，找到key，pass_ticket，appmsg_token保存起来一会儿要用。 ​ 然后选择URL为“/s/_biz=…”的url，看右侧选择“Raw”，获取微信PC端的cookie，和User-Agent保存起来，一会儿用。 三、代码实战​ 上边准备了web的User-Agent，cookies，token，fakeid与微信PC端的key，pass_ticket，appmsg_token，cookie,User-Agent这些信息来作为爬取数据的支撑，若爬取过程中中断，试试换一换key值就可以继续爬取了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211# -*- coding: utf-8 -*-import requestsimport timeimport jsonfrom openpyxl import Workbook#from pymongo import MongoClient # 公众号名称sheetName = \"\" # 目标urlurl = \"https://mp.weixin.qq.com/cgi-bin/appmsg\" # 填写从web页面获取的cookiesCookie = \"\"# 使用Cookie，跳过登陆操作headers = &#123; \"Cookie\": Cookie, \"User-Agent\": \" \", # 填写从web页面获取的User-Agent&#125; \"\"\"需要提交的data以下个别字段是否一定需要还未验证。注意修改yourtoken,numbernumber表示从第number页开始爬取，为5的倍数，从0开始。如0、5、10……token可以使用Chrome自带的工具进行获取fakeid是公众号独一无二的一个id，等同于后面的__biz\"\"\"# 填写web上边获取的token与fakeidtoken = \"\"fakeid =\"\"# type在网页中会是10，但是无法取到对应的消息link地址，改为9就可以了type = '9'data1 = &#123; \"token\": token, \"lang\": \"zh_CN\", \"f\": \"json\", \"ajax\": \"1\", \"action\": \"list_ex\", \"begin\": \"365\", \"count\": \"5\", \"query\": \"\", \"fakeid\": fakeid, \"type\": type,&#125; # 毫秒数转日期def getDate(times): # print(times) timearr = time.localtime(times) date = time.strftime(\"%Y-%m-%d %H:%M:%S\", timearr) return date # 获取阅读数和点赞数def getMoreInfo(link): # 获得mid,_biz,idx,sn 这几个在link中的信息 mid = link.split(\"&amp;\")[1].split(\"=\")[1] idx = link.split(\"&amp;\")[2].split(\"=\")[1] sn = link.split(\"&amp;\")[3].split(\"=\")[1] _biz = link.split(\"&amp;\")[0].split(\"_biz=\")[1] # fillder 中取得一些不变得信息 #req_id = \"\" # 填写从PC端微信获取的pass_ticket，appmsg_token,cookie,User-Agent pass_ticket = \"\" appmsg_token = \"\" # 目标url url = \"http://mp.weixin.qq.com/mp/getappmsgext\" # 添加Cookie避免登陆操作，这里的\"User-Agent\"最好为手机浏览器的标识 phoneCookie = \"\" headers = &#123; \"Cookie\": phoneCookie, \"User-Agent\": \"\" &#125; # 添加data，`req_id`、`pass_ticket`分别对应文章的信息，从fiddler复制即可。 data = &#123; \"is_only_read\": \"1\", \"is_temp_url\": \"0\", \"appmsg_type\": \"9\", 'reward_uin_count':'0' &#125; \"\"\" 添加请求参数 __biz对应公众号的信息，唯一 mid、sn、idx分别对应每篇文章的url的信息，需要从url中进行提取 key、appmsg_token从fiddler上复制即可 pass_ticket对应的文章的信息，也可以直接从fiddler复制 \"\"\" params = &#123; \"__biz\": _biz, \"mid\": mid, \"sn\": sn, \"idx\": idx, \"key\": \"\", \"pass_ticket\": pass_ticket, \"appmsg_token\": appmsg_token, \"uin\": \"MjgxOTMwODUzOQ%3D%3D\", \"wxtoken\": \"777\", &#125; # 使用post方法进行提交 content = requests.post(url, headers=headers, data=data, params=params).json() # 提取其中的阅读数和点赞数 #print(content[\"appmsgstat\"][\"read_num\"], content[\"appmsgstat\"][\"like_num\"]) try: readNum = content[\"appmsgstat\"][\"read_num\"] print(readNum) except: readNum=0 try: likeNum = content[\"appmsgstat\"][\"like_num\"] print(likeNum) except: likeNum=0 try: comment_count = content['comment_count'] print(\"true:\" + str(comment_count)) except: comment_count=0 print(\"error:\"+str(comment_count)) # 歇3s，防止被封 time.sleep(5) return readNum, likeNum,comment_count # 最大值365，所以range中就应该是73,15表示前3页def getAllInfo(url, begin): # 拿一页，存一页 messageAllInfo = [] # begin 从0开始，365结束 data1[\"begin\"] = begin # 使用get方法进行提交 content_json = requests.get(url, headers=headers, params=data1, verify=False).json() time.sleep(10) # 返回了一个json，里面是每一页的数据 if \"app_msg_list\" in content_json: for item in content_json[\"app_msg_list\"]: # 提取每页文章的标题及对应的url url = item['link'] # print(url) readNum, likeNum ,comment_count= getMoreInfo(url) info = &#123; \"title\": item['title'], \"readNum\": readNum, \"likeNum\": likeNum, 'comment_count':comment_count, \"digest\": item['digest'], \"date\": getDate(item['update_time']), \"url\": item['link'] &#125; messageAllInfo.append(info) # print(messageAllInfo) return messageAllInfo'''写入txt'''def writeTxt(text_list): fileName='test.txt' for lists in text_list: with open(fileName,'a',encoding='utf-8') as fh: fh.write(lists['date']+\"\\t\"+lists['title']+\"\\t\"+str(lists['readNum'])+\"\\t\"+str(lists['likeNum'])+\"\\t\"+str(lists['comment_count'])+\"\\t\"+lists['digest']+\"\\n\")def main(): # messageAllInfo = [] # 爬10页成功，从11页开始 for i in range(3,5): begin = i * 5 messageAllInfo = getAllInfo(url, str(begin)) print(\"第%s页\" % i) writeTxt(messageAllInfo) # putIntoMogo(messageAllInfo) def exportExcel(fileName): excel_QA = Workbook() # 建立一个工作本 sheet = excel_QA.active # 激活sheet sheet.title = sheetName # 对sheet进行命名 sheet.cell(1, 1).value = '推送日期' sheet.cell(1, 2).value = '推送标题' sheet.cell(1, 3).value = '阅读数' sheet.cell(1, 4).value = '点赞数' sheet.cell(1, 5).value = '评论数' fr = open(fileName,encoding='UTF-8-sig') count = 2 for each in fr.readlines(): sheet.cell(count, 1).value = each.strip().split(\"\\t\")[0] sheet.cell(count, 2).value = each.strip().split(\"\\t\")[1] sheet.cell(count, 3).value = each.strip().split(\"\\t\")[2] sheet.cell(count, 4).value = each.strip().split(\"\\t\")[3] sheet.cell(count, 5).value = each.strip().split(\"\\t\")[4] count += 1 excel_QA.save(sheetName + \".xlsx\")#保存 if __name__ == '__main__': main() exportExcel(\"./test.txt\") 其实爬虫涉及到好多内容，分布式爬虫，多线程，多进程等。","categories":[],"tags":[]},{"title":"支持向量机（Support Vector Machines）","slug":"SVM","date":"2019-03-18T16:00:00.000Z","updated":"2019-03-25T07:03:40.998Z","comments":true,"path":"2019/03/19/SVM/","link":"","permalink":"http://yoursite.com/2019/03/19/SVM/","excerpt":"","text":"一、算法理论1.支持向量机算法剧情介绍​ 支持向量机算法为什么叫这个呢？是因为其中的支撑向量，之所以叫“机”是因为它进行了分类。举一个例子来解释或许会更加清晰。 ​ 假如我们有一堆数据，这堆数据有两个特征X1,X2，我们把这一堆特征绘制到二维坐标系下，我们一看，这堆数据被分成了两波，我们可以用中间一条直线把这两波数据分开，其实支持向量机就是要找到一条直线把这一堆数据分开成两类。那还不简单，就是求一条直线划分这两个类嘛。但是支持向量机还有其他事情要做，就是提高泛化能力，我们怎样才能保证用一条线分割开，来了新数据进行预测类别，会更加精确呢，所以我们要求出一条最优的直线，那么我们应该对两组数据用最大间隔来分开，其中在最大间隔边上的向量称之为支撑向量。恰好在最大间隔最中间的位置把两波数据分离开，使得新的数据来尽可能分类正确。这个就是硬间隔分类器。 ​ 有了硬间隔分类器，那么与之对应的还有软间隔分类器，那么什么是软间隔分类器呢？因为在现实的数据中集中肯定含有噪声数据落最大间隔里边，数据激励分类线的较近地方，我们来添加松弛变量来允许部分数据可以在分类间隔内，这就是软间隔分类器。 ​ 其中还有一类是核函数分类器，假如我们有一组数据，分类间隔应该是圆形的曲线才行，我们用线性间隔分类的支持向量机肯定是不行的，所以我们就采用带有核函数的非线性分类器。其实采用核方法分类器是把处于低维空间的数据升到更高的维度，结果呈现出来的就是线性可分的了。 2.获取相关数据​ 在支持向量机中，数据的标签类Y一般为+1与-1。数据样例如下： 序号 X1 X2 Y 1 0.227222 0.527437 -1 2 0.759290 0.330720 -1 … … … … 100 -0.021605 0.158080 1 3.公式原理​ 接下啦我们将一一解释各个支持向量机中的概念。 超平面​ 如2中表格里的数据集，我们可以确定一条直线把一堆数据用一条直线分开，这条直线就是超平面。直线的公式如下：$$w_{1}x_{1}+w_{2}x_{2}+…+w_{n}x_{n}+b=0$$也可以表示为：$$W^{T}x+b=0$$其中这里的W为权重，x为样本特征，b为偏移参数。 分类决策函数​ 线性分类决策函数为：$$f(x)=sign(W^{T}x+b)$$这个函数的性质是值域在（-1，1），而且定义域为无穷，在f(0)=0，其中f(x)&gt;0,则为正类，f(x)&lt;0为负类，因此可以对数据集进行二分类。可以对比下逻辑回归，那个值域是（0,1）。 函数间隔​ 在超平面确定的情况下，我们的有一样本点离超平面越远，我们就越确定该样本会被正确分类，因此我们定义了表示离超平面远近的函数间隔：$$\\hat{r_{i}} = |W^{T}x+b|=y_{i}(W^{T}x+b)\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$来表示是否能被正确分类的确信度。 几何间隔​ 上述函数间隔有一缺陷是，当W与b成比例增加或者减小时，函数间隔也会成比例变化，然而超平面并没有发生变化。因此我们对W加上约束使得间隔确定。这个就是几何间隔：$$r_{i} = \\frac{y_{i}(Wx+b)}{||W||}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$此时我们成比例增加W和b，几何间隔就不会发生变化。我们可以得出几何间隔与函数间隔之间的关系：$$r_{i} = \\frac{\\hat{r_{i}}}{||W||}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 最大间隔​ 我们要尽量把数据分离的更好，有更好的泛化能力就要找到最大间隔，这个最大间隔就是各个数据点距离超平面的距离，其实就是寻找几何间隔最大值，下边是距离表示公式。$$distance = \\frac{|Wx+b|}{||W||}=r_{i}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$通过这个距离我们可以找到几何间隔最大值，这样就确定一条唯一的超平面，可以很好的对样本数据进行分类，而且对离超平面较近的数据点有很好的划分效果。我们要最大化几何间隔可以表示为：$$\\max_{W,b} r_{i}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$ST: y_{i}(W^{T}x_{i}+b)\\geq r\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 其中ri表示所有的几何间隔，r表示最小的几何间隔。根据几何间隔与函数间隔的关系，我们可以得出：$$\\max_{W,b} \\frac{\\hat{r_{i}}}{||W||}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$ST: y_{i}(W^{T}x_{i}+b)\\geq \\frac{\\hat{r_{i}}}{||W||}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 因为我们的W和b成比例增加，几何间隔不会变，函数间隔会成比例增加，且不会影响最优化的解，对目标函数优化没有影响，我们这里的函数间隔就取1，因此我们就得到：$$\\max_{w,b} \\frac{1}{||W||}$$ $$ST: y_{i}(W^{T}x_{i}+b)\\geq 1\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 那么，我们可以转化为：$$\\min_{w,b}\\frac{1}{2}||W||^{2}$$ $$ST:y_{i}(W^{T}x_{i}+b)\\geq 1\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 这里，从最大化转化到最小化这里是等价的。 凸二次规划​ 凸优化问题一般可以描述为：$$\\underset{w}{min}f(W)$$ $$ST:g_{i}(W)\\leq0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,k)$$ $$h_{i}(W)=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,l)$$ 凸函数很简单，若二阶导数在区间上非负则为凸函数，比如x^2，这个就是凸函数。其中凸优化问题中f,g,h在定义域上都是可微的。其中凸优化问题中目标函数f和约束条件g为凸函数，等式约束h为仿射函数（就是线性函数）。 ​ 凸二次规划为题是凸优化的特殊形式，当目标函数f是二次型函数且约束函数g是仿射函数，就被称为凸二次规划。一般形式为：$$\\underset{x}{min}\\frac{1}{2}x^{T}Qx+c^{T}x$$ $$ST:Wx\\leq b$$ 若 Q 为半正定矩阵，则上面的目标函数是凸函数，相应的二次规划为凸二次规划问题；此时若约束条件定义的可行域不为空，且目标函数在此可行域有下界，则该问题有全局最小值。若Q为正定矩阵，则该问题有唯一的全局最小值。 支持向量​ 在线性分类器中，离超平面最近的那些数据点称为支持向量，这些支持向量满足：$$y_{i}(W^{T}x_{i}+b)-1= 0$$其中这里的xi表示支持向量。 间隔边界​ 对于y=1的支持向量样本点满足：$$H1:W^{T}x_{i}+b= 1$$对于y=-1的支持向量样本点满足：$$H2:W^{T}x_{i}+b= -1$$这两个就是在正类与负类的间隔边界。其中Wx+b=0这条线在H1与H2之间，且距离这两条线距离相等，这里就是我们最大几何间隔后的结果。即，两边的支持向量都距离超平面最大，因此就在中间了。从这里我们可以看出，对分类器超平面起决定性作用的只有支持向量这些样本点，去掉其他非支持向量样本点，超平面也不会改变。 对偶算法​ 通过对偶算法来求解线性支持向量机最优化问题，我们可以构建拉格朗日函数：$$L(W,b,a) = \\frac{1}{2}||W||^{2}-\\sum_{i=1}^{n}a_{i}(y_{i}(Wx_{i}+b)-1)\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$其中ai&gt;=0是拉格朗日乘子。$$\\Theta (W)=\\underset{a_{i}}{max}L(W,b,a)\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$当某个条件不满足时，有y(wx+b)&lt;1时，θ(W)最大值可以为无穷（a为无穷）。当所有条件都满足时：$$\\Theta (W) = \\frac{1}{2}||W||^{2}$$这个就是我们之前要最小化的数据量。在要求约束条件都满足的情况下最小化上式子其实等价于最小化θ(W)。即此时的目标函数为：$$p’ = \\underset{W,b}{min}\\Theta (W)=\\underset{W,b}{min}\\,\\underset{a_{i}}{max}L(W,b,a)$$其中p*为现在目标函数最优解，我们现在把最大化与最小化互换位置：$$d’=\\underset{a_{i}}{max}\\,\\underset{W,b}{min}L(W,b,a)$$这里我们可以得出d’&lt;=p’，因为在最小值的范围里边取最大值肯定小于或等于最大值里取最小值。这就好比“宁做鸡头，不做凤尾”，其实凤尾还是好的。 KTT条件​ 上述的p’与d’什么时候相等呢？就是在满足KTT条件的情况下p’=d’。我们可以选择d’的这个式子，先求最小值，即：$$\\underset{W,b}{min}L(W,b,a)$$我们分别对W与b求偏导数置0得：$$\\frac{\\partial L(W,b,a)}{\\partial W}=W-\\sum_{i=1}^{n}a_{i}y_{i}x_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$\\frac{\\partial L(W,b,a)}{\\partial b}=\\sum_{i=1}^{n}a_{i}y_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 得出：$$W=\\sum_{i=1}^{n}a_{i}y_{i}x_{i}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$\\sum_{i=1}^{n}a_{i}y_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 将上述公式带入到拉格朗日函数中去，得出：$$L(W,b,a)=-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}(x_{i}\\cdot x_{j})+\\sum_{i=1}^{n}a_{i}$$接下来求L的最大值得出：$$\\underset{a_{i}}{max}\\sum_{i=1}^{n}a_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}(x_{i}\\cdot x_{j})$$ $$ST:\\sum_{i=1}^{n}a_{i}y_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$a_{i}\\geq 0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 目标函数最大化转化为最小化得出：$$\\underset{a_{i}}{min}\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}(x_{i}\\cdot x_{j})-\\sum_{i=1}^{n}a_{i}$$ $$ST:\\sum_{i=1}^{n}a_{i}y_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$a_{i}\\geq 0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 通过以上函数，我们可以计算出n个最优的a分别为：a1,a2,…,an。然后把a分别带入上边的权重W代数式子，求得每个最优W：w1,w2,…,wn和b。 ​ 我们要对数据点x进行分类的话，我们可以带入f(x)=Wx+b的出结果，然后根据正负就可以进行分类了。我们把W带入f(x)得出：$$f(x) = \\sum_{i=1}^{n}(a_{i}x_{i}y_{i})^{T}x+b=\\sum_{i=1}^{n}a_{i}y_{i}&lt;x_{i},x&gt;+b$$其中&lt;xi,x&gt;表示内积。对于对新的样本进行分类只需要就算新样本与训练数据点的内积，在这里我们实际采用计算的实际只有支持向量参与了计算，对于非支持向量数据点我们不参与计算，非支持向量的ai值全部为0，为什么会为0呢？之前就说过，非支持向量对超平面确定不产生影响。对于之前的拉格朗日函数，我们要求出最大值：$$\\underset{a_{i}}{max}L(W,b,a) = \\underset{a_{i}}{max}\\frac{1}{2}||W||^{2}-\\sum_{i=1}^{n}a_{i}(y_{i}(Wx_{i}+b)-1)\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$对于这个式子，如果xi为支持向量的话，减号后边一项全部为0，如果xi为非支持向量的话，ai为0，这样就可以获得最大值。 松弛变量​ 一般在实际的训练环境中，我们遇到的数据会存在特异的点，如果将这些特异的点给去除，那么这些数据就线性可分了。如果存在这些特异点，就意味着不能满足函数间隔大于等于1的条件，为了解决这个问题，我们引入了松弛变量ξ&gt;=0，使得函数间隔加上松弛变量大于等于1就行。我们的约束条件就变成了：$$y_{i}(W^{T}x_{i}+b)+\\xi_{i} \\geq 1\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$这里的ξ不可以任意大，如果任意大的话所有超平面都符合条件了。所以我们的上边目标函数有：$$\\frac{1}{2}||W||^{2}$$变为：$$\\frac{1}{2}||W||^{2}+C\\sum_{i=1}^{n}\\xi {i}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$其中C&gt;0为惩罚系数，C越大，对误分类惩罚越大，C越小对误分类惩罚越小。因此我们得到最优化问题：$$\\underset{W,b,\\xi}{min} \\frac{1}{2}||W||^{2}+C\\sum{1}^{n}\\xi _{i}$$ $$ST:y_{i}(W^{T}x_{i}+b)+\\xi_{i} \\geq 1\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$\\xi_{i} \\geq 0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 在这里最小化是间隔最小化与误分类点个数尽量小。这里W的解释唯一的，但是b的值就不是唯一的，在一个区间内。我们沿用之前的方法写出最优化拉格朗日函数：$$L(W,b,\\xi ,a,u)=\\frac{1}{2}||W||^{2}+c\\sum_{1}^{n}\\xi {i}-\\sum{1}^{n}a_{i}(y_{i}(Wx_{i}+b)-1+\\xi {i})-\\sum{1}^{n}u _{i}\\xi {i}$$首先对W，b，ξ求导，求最小得出：$$W=\\sum{i=1}^{n}a_{i}y_{i}x_{i}\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$\\sum_{i=1}^{n}a_{i}y_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ $$\\frac{\\partial L}{\\partial \\xi}=0\\Rightarrow C-a_{i}-u_{i}=0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 带入拉格朗日函数得出：$$\\underset{a}{max}\\sum_{i=1}^{n}a_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}(x_{i}\\cdot x_{j})$$ $$ST:\\sum_{i=1}^{n}a_{i}y_{i}=0$$ $$C-a_{i}-u_{i}=0$$ $$a_{i}\\geq 0$$ $$u_{i}\\geq 0\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 这里我们可以得出ai&lt;=C。因此对偶问题可以写为：$$\\underset{a}{max}\\sum_{i=1}^{n}a_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}&lt;x_{i}\\cdot x_{j}&gt;$$ $$ST:\\sum_{i=1}^{n}a_{i}y_{i}=0$$ $$0\\leq a_{i}\\leq C\\, \\, \\,\\, \\, \\, i\\in (1,2,…,n)$$ 和之前的对比发现，这里的约束条件ai受到C的制约。然后就和上边的解法一样依次求出a，W，b。b在这里是求出多个b的平均值。 核技巧​ 当输入数据是线性不可分的，我们可以通过核函数来学习非线性支持向量机，等价于隐式的在高维空间线性可分。 ​ 对于由两个不同粗数据围成的内外圆，我们不能用直线线性划分，我们要映射到高维空间中，我们可以可以升维：$$a_{1}X_{1}+a_{2}X_{1}^{2}+a_{3}X_{2}+a_{4}X_{2}^{2}+a_{5}X_{1}X_{2}+a6=0$$我们从二维升到了五维，其可以表示为：$$Z_{1}=X_{1}，Z_{2}=X_{1}^{2},Z_{3}=X_{2},Z_{4}=X_{2}^{2},Z_{5}=X_{1}X_{2}$$我们可以表示为：$$\\sum_{i=1}^{n}a_{i}Z_{i}+a_{6}=0$$利用核方法分为两步：首先把数据从低维空间映射到高维空间去，然后用线性分类方法去解决。当我们拿到线性不可分的数据的时候我们可以可以把数据映射到更高的维度中去，比如说，二维可以映射到五维，三维就可以映射到十九维，当特征多的时候就可以映射到无穷维，这时计算量很大，处理起来很复杂，此时我们就需要核函数来进行解决。 ​ 其中核技巧的思想是：定义核函数K（x,z），不显示的使用映射函数把数据从低维空间映射到高维空间，然后求内积去计算。通常情况下，计算K（x,z）比较容易，而升维之后在计算比较复杂。因此我们可以通过核函数来进行映射处理，此时对偶问题的目标函数是：$$\\underset{a}{max}\\sum_{i=1}^{n}a_{i}-\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{i}a_{j}y_{i}y_{j}K&lt;x_{i}\\cdot x_{j}&gt;$$然后分类决策函数也可以用核函数来处理：$$f(x)=sign(\\sum_{j=1}^{n}a_{i}y_{i}K&lt;x_{i}\\cdot x_{j}&gt;+b)$$ SMO​ 在最优化求解a的时候，我们有n个变量的a需要在目标函数中求出，直接去优化很复杂，如果我们每次选择两个变量比如a1和a2去优化，其它的都视为常数。因为a1y1+a2y2+…+anyn=0，我们取a1，a2进行优化其余固定看作常量，那么将转化为二值优化问题。我们可以对上述的最优化求a的式子变形：$$\\underset{a1,a2}{min}\\frac{1}{2}K_{11}a_{1}^{2}+\\frac{1}{2}K_{22}a_{2}^{2}+K_{12}a_{1}a_{2}-(a_{1}+a_{2})+y_{1}a_{1}\\sum_{i=3}^{n}y_{i}a_{i}K_{i1}+\\sum_{i=3}^{n}y_{i}a_{i}K_{i2}$$ $$ST:a_{1}y_{1}+a_{2}y_{2}=-\\sum_{i=3}^{n}y_{i}a_{i}=\\zeta$$ $$0\\leq a_{i}\\leq C\\, \\, \\, \\, \\, \\, i=1,2$$ 其中Kij=φ(xi).(xj)，我们需要对满足约束太监下求最小的a1与a2。根据上边的约束条件，又因为y1与y2取值为1或-1，这样a1与a2均落在[0，C]*[0，C]的正方形盒子里，而且两条关系直线的斜率为1或者-1，那么我们可以得出两种情况：$$y_{1}=y_{2}:a_{1}+a_{2}=\\zeta$$ $$y_{1}\\neq y_{2}:a_{1}-a_{2}=\\zeta$$ 因此我们可以化简式子为求关于a1的式子，这里就可以得出式子：$$a_{1}=y_{2}(\\zeta-a_{2}y_{2})$$我们把这个式子带入目标函数就得到关于a1的二次函数，这里就得到了二次函数求极值的问题了，我们只要确定a1的定义域就可以。由于0&lt;=a1&lt;=C，并且0&lt;=y2(ζ-a2y2)&lt;=C，就可以得到a1的可行域。在这个可行域求解二次函数极值。 4.分析判断​ SVM通过找到超平面来对数据进行划分分类，为了提高算法的鲁棒性，我们选择有最大间隔的超平面，然后对新来的样本数据点进行分类。其中包含有硬间隔分类器，软间隔分类器和核方法分类器，硬间隔分类器不允许任何样本点出现在分类间隔内，但是实际样本中总存在一部分噪声数据，因此我们设计了软间隔分类器，可以允许少部分噪声点落在分类间隔内部。但当我们遇到线性不可分的数据的时候，我们又无可奈何了，这也是比较常见的，我们就采用核方法对样本升维，在高的维度样本点数据就变成线性可分的了。 二、算法实践​ 这次我们实现了SVM版的手写识别，其中我们采用了SMO算法加速了对SVM算法求解过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424import numpy as npfrom os import listdir'''解析数据'''def loadDataSet(fileName): dataMat = [] labelMat = [] fr = open(fileName) for line in fr.readlines(): lineArr = line.strip().split('\\t') dataMat.append([float(lineArr[0]),float(lineArr[1])]) labelMat.append(float(lineArr[2])) return dataMat,labelMat'''选择不等于输入alpha的alphai: alpha下标m: alpha数目'''def selectJrand(i,m): j = i while (j==i): j = int(np.random.uniform(0,m)) return j'''H,L： 限制alpha的最大值与最小值'''def clipAlpha(aj,H,L): if aj &gt; H: aj = H if L &gt; aj: aj = L return aj'''SMO简化算法dataMatIn: 数据集classLabels: 类别标签C： 常数Ctoler: 容错率maxIter： 取消前最大的循环次数'''def smoSimple(dataMatIn,classLabels,C,toler,maxIter): dataMatrix = np.mat(dataMatIn) labelMat = np.mat(classLabels).transpose() b = 0 m,n = np.shape(dataMatrix) alphas = np.mat(np.zeros((m,1))) # 改变量存储了没有改变任何alpha变量的请款你改下遍历数据集的次数 # 当变量达到maxIter的时候，函数结束运行并退出 iter = 0 while (iter &lt; maxIter): # 记录alpha是否已经进行优化 alphaPairsChanged = 0 for i in range(m): # fXi计算我们预测的类别 Ei是与真实类别的误差 fXi = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b Ei = fXi - float(labelMat[i]) # 如果alpha可以更改进入优化过程（如果预测结果和真实结果误差很大，就对数据实例所对应的alpha进行优化） if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)): # 随机选择第二个alpha j = selectJrand(i,m) fXj = float(np.multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b Ej = fXj - float(labelMat[j]) alphaIold = alphas[i].copy() alphaJold = alphas[j].copy() # 保证alpha在0与c之间 if (labelMat[i] != labelMat[j]): L = max(0,alphas[j] - alphas[i]) H = min(C,C + alphas[j] - alphas[i]) else: L = max(0,alphas[j] + alphas[i] -C) H = min(C,alphas[j] + alphas[i]) if L == H: print(\"L == H\") continue # eta是alpha[j]的最优修改量 eta = 2.0 * dataMatrix[i,:] * dataMatrix[j,:].T - dataMatrix[i,:] * dataMatrix[i,:].T - dataMatrix[j,:] * dataMatrix[j,:].T if eta &gt;= 0: print(\"eta &gt;= 0\") continue alphas[j] -= labelMat[j]*(Ei - Ej)/eta alphas[j] = clipAlpha(alphas[j],H,L) if (abs(alphas[j] - alphaJold) &lt; 0.00001): print(\"j not moving enough\") continue # 对i进行修改，修改量与j相同，但方向相反 alphas[i] += labelMat[j]*labelMat[i]*(alphaJold-alphas[j]) # 设置常数项 b1 = b - Ei - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T b2 = b - Ej - labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 elif (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1 else: b = (b1 + b2)/2.0 alphaPairsChanged += 1 print(\"iter: %d i:%d,pairs changed %d\" % (iter,i,alphaPairsChanged)) if (alphaPairsChanged == 0): iter += 1 else: iter = 0 print(\"iteration number:%d\" % iter) return b,alphas'''核转换函数'''def kernelTrans(X,A,KTup): m,n = np.shape(X) K = np.mat(np.zeros((m,1))) if KTup[0] == 'lin': K = X * A.T elif KTup[0] == 'rbf': for j in range(m): deltaRow = X[j,:] - A K[j] = deltaRow*deltaRow.T K = np.exp(K /(-1*KTup[1]**2)) else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized') return K''''''class optStruct: def __init__(self,dataMatIn,classLabels,C,toler,kTup): self.X = dataMatIn self.labelMat = classLabels self.C = C self.tol = toler self.m = np.shape(dataMatIn)[0] self.alphas = np.mat(np.zeros((self.m,1))) self.b = 0 self.eCache = np.mat(np.zeros((self.m,2))) # 误差缓存 self.K = np.mat(np.zeros((self.m,self.m))) for i in range(self.m): self.K[:,i] = kernelTrans(self.X,self.X[i,:],kTup) def calcEk(oS,k): fXk = float(np.multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T)) +oS.b Ek = fXk - float(oS.labelMat[k]) return Ek def selectJ(i,oS,Ei): maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1,Ei] validEcacheList = np.nonzero(oS.eCache[:,0].A)[0] if (len(validEcacheList)) &gt; 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS,k) deltaE = abs(Ei - Ek) if (deltaE &gt; maxDeltaE): maxk = k # 选择具有最大步长的j maxDeltaE = deltaE Ej = Ek return maxK,Ej else: j = selectJrand(i,oS.m) Ej = calcEk(oS,j) return j,Ej def updateEk(oS,k): Ek = calcEk(oS,k) oS.eCache[k] = [1,Ek]'''寻找决策边界的优化例程'''def innerL(i,oS): Ei =calcEk(oS,i) if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): # 第二个alpha选择中的启发式方法 j,Ej = selectJ(i,oS,Ei) alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if(oS.labelMat[i] != oS.labelMat[j]): L = max(0,oS.alphas[j] - oS.alphas[i]) H = min(oS.C,oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0,oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C,oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L == H\") return 0 eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T if eta &gt;= 0: print(\"eta&gt;=0\") return 0 oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta oS.alphas[j] = clipAlpha(oS.alphas[j],H,L) updateEk(oS,j) if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j]) updateEk(oS,i) b1 = oS.b - Ei - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[i,:]*oS.X[j,:].T b2 = oS.b - Ej - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[j,:]*oS.X[j,:].T if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1 elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 '''引入核函数后，寻找决策边界的优化例程'''def kernelInnerL(i,oS): Ei =calcEk(oS,i) if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)): # 第二个alpha选择中的启发式方法 j,Ej = selectJ(i,oS,Ei) alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if(oS.labelMat[i] != oS.labelMat[j]): L = max(0,oS.alphas[j] - oS.alphas[i]) H = min(oS.C,oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0,oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C,oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L == H\") return 0 eta = 2.0 * oS.K[i,j] - oS.X[i,i] - oS.K[j,j] if eta &gt;= 0: print(\"eta&gt;=0\") return 0 oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta oS.alphas[j] = clipAlpha(oS.alphas[j],H,L) updateEk(oS,j) if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j]) updateEk(oS,i) b1 = oS.b - Ei - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.K[i,j] b2 = oS.b - Ej - oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j] - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.K[j,j] if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1 elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 '''引入核函数后'''def kernelCalcEk(oS,k): fXk = float(np.multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ekdef smoP(dataMatIn,classLabels,C,toler,maxIter,kTup=('lin',0)): oS = optStruct(np.mat(dataMatIn),np.mat(classLabels).transpose(),C,toler,kTup) iter = 0 entireSet = True alphaPairsChanged = 0 while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: for i in range(oS.m): alphaPairsChanged += innerL(i,oS) print(\"fullSet,iter: %d i:%d,pairs changed %d\" % (iter,i,alphaPairsChanged)) else: nonBoundIs = np.nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0] for i in nonBoundIs: alphaPairsChanged += innerL(i,oS) print(\"non-bound,iter: %d i:%d,pairs changed %d\" % (iter,i,alphaPairsChanged)) iter += 1 if entireSet: entireSet = False elif (alphaPairsChanged == 0): entireSet = True print(\"iteration number: %d\" % iter) return oS.b,oS.alphas'''根据alpha 计算权重w'''def calcWs(alphas,dataArr,classLabels): X = np.mat(dataArr) labelMat = np.mat(classLabels).transpose() m,n = np.shape(X) w = np.zeros((n,1)) for i in range(m): w += np.multiply(alphas[i]*labelMat[i],X[i,:].T) return w'''利用核函数进行分类的径向基测试函数'''def testRbf(k1=1.3): dataArr,labelArr = loadDataSet('./data/Ch06/testSetRBF.txt') b,alphas = smoP(dataArr,labelArr,200,0.0001,10000,('rbf',k1)) dataMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() svInd = np.nonzero(alphas.A &gt; 0)[0] sVs = dataMat[svInd] labelSV = labelMat[svInd] print(\"there are %d Support Vectors\" % np.shape(sVs)[0]) m,n = np.shape(dataArr) errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs,dataMat[i,:],('rbf',k1)) predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 print(\"训练错误率是:%f\" % (float(errorCount)/m)) dataArr,labelArr = loadDataSet('./data/Ch06/testSetRBF2.txt') errorCount = 0 dataMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() m,n = np.shape(dataMat) for i in range(m): kernelEval = kernelTrans(sVs,dataMat[i,:],('rbf',k1)) predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 print(\"错误率是：%f\" % (float(errorCount)/m))'''手写识别系统图像转向量'''def img2vector(filename): returnVect = np.zeros((1,1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect'''加载图片数据，我们这里只加载1和9的数据进行二分类'''def loadImages(dirName): hwLabels = [] trainingFileList = listdir(dirName) m = len(trainingFileList) trainingMat = np.zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) if classNumStr == 9: hwLabels.append(-1) else: hwLabels.append(1) trainingMat[i,:] = img2vector('%s/%s' % (dirName,fileNameStr)) return trainingMat,hwLabels'''手写识别系统测试'''def testDigits(kTup=('rbf',10)): dataArr,labelArr = loadImages('./data/Ch06/digits/trainingDigits') b,alphas = smoP(dataArr,labelArr,200,0.0001,10000,kTup) dataMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() svInd = np.nonzero(alphas.A&gt;0)[0] sVs = dataMat[svInd] labelSV = labelMat[svInd] print(\"there are %d support vectors\" % np.shape(sVs)[0]) m,n = np.shape(dataMat) errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs,dataMat[i,:],kTup) predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 print(\"the training error rate is: %f\" % (float(errorCount)/m)) dataArr,labelArr = loadImages('./data/Ch06/digits/testDigits') errorCount = 0 dataMat = np.mat(dataArr) labelMat = np.mat(labelArr).transpose() m,n = np.shape(dataMat) for i in range(m): kernelEval = kernelTrans(sVs,dataMat[i,:],kTup) predict = kernelEval.T * np.multiply(labelSV,alphas[svInd]) + b if np.sign(predict) != np.sign(labelArr[i]): errorCount += 1 print('the test error rate is: %f' % (float(errorCount)/m))# 手写识别系统svm测试testDigits(('rbf',20)) 三、后记​ SVM算法是一个比较复杂的算法，其实还有好多推到，后期再进行部分细节补充。下一章介绍adaBoost算法。","categories":[],"tags":[]},{"title":"逻辑回归（Logistics Regression）","slug":"LogisticsRegression","date":"2019-03-13T16:00:00.000Z","updated":"2019-03-15T03:22:08.125Z","comments":true,"path":"2019/03/14/LogisticsRegression/","link":"","permalink":"http://yoursite.com/2019/03/14/LogisticsRegression/","excerpt":"","text":"一、算法理论1.逻辑回归算法剧情介绍​ 逻辑回归看上去貌似是解决的回归问题，其实不是的，它主要解决的是分类问题。其中这里的回归就是指的拟合出的对数据点进行分类的曲线。那么如何进行分类呢？我们就用到了阶跃函数sigmoid()，这个函数很特殊，值的范围在（0,1），通过0.5这个值来分割，那么就得到了分类效果。举一个例子来说明： ​ 假如，有一匹马得了氙气病症，要预测下马的死亡率。假设我们得到了以往的这种病症的好多匹马的病史，其中含有21个特征X1,…,X21，结果只有两个就是是存活与死亡。从医生的角度，我们就会知道，医生会看这匹马的各个特征，然后根据这些指标的情况，根据自己历史经验最后的出一个结论。逻辑回归算法就可以做到，通过训练样本学习权重参数，然后通过阶跃函数sigmoid（）把值缩小到（0,1）这个范围，然后设定一个阈值，把值给划分为两类，最终与测试马匹是否会死亡。 ​ 这里学习权重参数不像贝叶斯算法的那个概率了，我们需要用到梯度上升优化算法，和梯度下降一样，只不过一个是加一个是减。通过不断更新权重，最后获得最佳分割曲线进行分类。 2.获取相关数据​ 假如我们获得了马匹的病症数据集，特征为X1,…,X20，类别为存活0，死亡1。如下： 序号 X1 X2 X3 … X20 X21 类别 1 2.0 38.5 66.0 … 0.0 0.0 0 2 1.0 1.0 39.2 … 2.0 2.0 0 3 2.0 1.0 38.3 … 0.0 0.0 1 4 1.0 9.0 39.1 … 3.0 5.3 0 … … … … … … … … … … … … … … … … 297 1.0 1.0 37.5 … 0.0 0.0 0 298 1.0 1.0 36.5 … 3.0 3.4 1 299 1.0 1.0 37.2 … 1.0 1.0 0 3.公式原理sigmoid函数​ 首先我们要根据特征建立sigmoid的输入函数:$$z = w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+…+w_{n}x_{n}$$其中X0=1，接下来是阶跃函数sigmoid()函数：$$\\sigma \\left ( z \\right )=\\frac{1}{1+e^{-z}}$$这个函数的值域是(0,1),在Z=0时，值取0.5。那么我们如何更新权重参数呢？那就是梯度上升算法。 梯度上升算法​ 梯度上升的思想就是，找到某函数的最大值，最好的方法就是沿着梯度方向去探索。用▽表示梯度，对于f(x,y)的梯度计算可以表示为：$$\\triangledown f(x,y) = \\begin{bmatrix}\\frac{\\partial f(x,y)}{\\partial x}\\\\frac{\\partial f(x,y)}{\\partial y}\\end{bmatrix}$$其中，对x，y求偏导，分别是向x和y方向移动，函数只要在计算点有定义，可微即可。那么我们的权重迭代公式为:$$w: = w+\\alpha \\triangledown _{w}f(w)$$其中，这里的alpha指的是移动步长（学习率）。这里把加号变为减号就是梯度下降算法。一般我们把每个权重参数在最开始全部初始化为1，然后重复训练，找到决策边界最佳曲线参数就可以了。 随机梯度上升算法​ 我们可以看到上边采用梯度上升算法进行权重更新，这样更新效率是不是特别慢，更新一次权重的值我们要遍历整个数据集，当数据集庞大的时候，权重的更新会很慢，收敛很慢。因此我们采用随机梯度上升算法来进行对权重快速更新。 ​ 随机梯度上升思想是：一次仅用数据集中的一个点来更新权重参数，而且这个数据点是在样本集中随机抽取，抽取后下次更新权重排除上次随机抽取到的样本。 ​ 一个很直观的方面可以比较效率，假如有100个样本，有两个特征（加上X0为三个）和三个权重参数。随机梯度上升算法要做300次乘法计算，然而随机梯度上升算法每次只需计算3次乘法，很明显提高了效率。 4.分析预测​ 我们就需要通过样本集来进行训练，然后训练得到权重参数后，我们就可以把新的样本集带入进来求得一个在，通过sigmoid()函数求解一个在（0,1）内的值，这个值可以通过0.5这个阈值进行区分，就预测到这个样本的类别了。 二、算法实践​ 这里我们实现了一个案例，是从氙气病症预测病马的死亡率。其中我们得到的原始数据集中存在空值，这里我们全补为0,特征值补为0的时候不影响权重参数的更新。我们这里采用的误差方案是真实值减去预测值。并且更新权重采用 新权重=旧权重x学习率x（真实值-预测值）x特征值，并且学习率我们进行动态更新，收敛更快。我们这里采用的是随机梯度上升算法进行优化。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import numpy as npimport matplotlib.pyplot as plt'''定义sigmoid函数inX: 关系式的结果'''def sigmoid(inX): return 1.0/(1+np.exp(-inX))'''梯度上升dataMatIn: 数据矩阵classLabels: 标签矩阵'''def gradAscent(dataMatIn,classLabels): dataMatrix = np.mat(dataMatIn) labelMat = np.mat(classLabels).transpose() # 转置操作 m,n = np.shape(dataMatrix) alpha = 0.001 maxCycles = 500 weights = np.ones((n,1)) for k in range(maxCycles): h = sigmoid(dataMatrix*weights) error = (labelMat - h) weights = weights + alpha * dataMatrix.transpose()*error return weights'''画出拟合直线wei: 权重'''def plotBestFit(weights): dataMat,labelMat = loadDataSet() dataArr = np.array(dataMat) n = np.shape(dataArr)[0] xcord1 = [] ycord1 = [] xcord2 = [] ycord2 = [] for i in range(n): if int(labelMat[i]) == 1: xcord1.append(dataArr[i,1]) ycord1.append(dataArr[i,2]) else: xcord2.append(dataArr[i,1]) ycord2.append(dataArr[i,2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1,ycord1,s=30,c='red',marker='s') ax.scatter(xcord2,ycord2,s=30,c='green') x = np.arange(-3.0,3.0,0.1) y = (-weights[0]-weights[1]*x)/weights[2] # W0X0+W1X1+W2X2 = 0的变形，X0=1 ax.plot(x,y) plt.xlabel('X1') plt.ylabel('X2') plt.show()# 随机梯度上升算法def stocGradAscent0(dataMatrix,classLabels): m,n = np.shape(dataMatrix) alpha = 0.01 weights = np.ones(n) for i in range(2000): for i in range(m): h = sigmoid(np.sum(dataMatrix[i]*weights)) error = classLabels[i] - h weights = weights + alpha * error * dataMatrix[i] return weights# 改进随机梯度上升算法def stocGradAscent1(dataMatrix,classLabels,numIter = 150): m,n = np.shape(dataMatrix) weights = np.ones(n) for j in range(numIter): dataIndex = list(range(m)) for i in range(m): alpha = 4/(1.0+j+i) + 0.001 # 每次迭代调整alpha # 随机选取样本更新权重 randIndex = int(np.random.uniform(0,len(dataIndex))) h = sigmoid(sum(dataMatrix[randIndex]*weights)) error = classLabels[randIndex] - h weights = weights + alpha * error * dataMatrix[randIndex] del(dataIndex[randIndex]) return weights'''Logistic回归分类函数inX: 待测样本weights: 训练后的权重'''def classifyVector(inX,weights): prob = sigmoid(sum(inX*weights)) if prob &gt; 0.5: return 1.0 else: return 0.0'''训练测试函数'''def colicTest(): frTrain = open('horseColicTraining.txt') frTest = open('horseColicTest.txt') trainingSet = [] trainingLabels = [] for line in frTrain.readlines(): currLine = line.strip().split('\\t') lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) trainingSet.append(lineArr) trainingLabels.append(float(currLine[21])) trainWeights = stocGradAscent1(np.array(trainingSet),trainingLabels,500) errorCount = 0 numTetsVec = 0.0 for line in frTest.readlines(): numTetsVec += 1.0 currLine = line.strip().split('\\t') lineArr = [] for i in range(21): lineArr.append(float(currLine[i])) if int(classifyVector(np.array(lineArr),trainWeights)) != int(currLine[21]): errorCount += 1 errorRate = (float(errorCount)/numTetsVec) print(\"错误率是:%f\" % errorRate) return errorRate''''''def multiTest(): numTests = 10 errorSum = 0.0 for k in range(numTests): errorSum += colicTest() print(\"这%d次的平均错误率是:%f\" % (numTests,errorSum/float(numTests))) # 从氙气病症预测病马的死亡率multiTest() 三、后记​ 这里说了逻辑回归的一些内容，其中包含了梯度上升（下降）的优化算法，之后会有单独的章节来讨论梯度优化算法。","categories":[],"tags":[]},{"title":"贝叶斯（Bayes）","slug":"bayes","date":"2019-03-12T16:00:00.000Z","updated":"2019-03-15T03:28:29.476Z","comments":true,"path":"2019/03/13/bayes/","link":"","permalink":"http://yoursite.com/2019/03/13/bayes/","excerpt":"","text":"一、算法理论1.贝叶斯算法剧情介绍​ 贝叶斯分类器其实就是通过已知的数据得出先验概率和条件概率，然后来求解后验概率的过程。什么是先验概率？什么是条件概率？什么是后验概率？我们在下边例子中来进行解答。 ​ 假如，这里有10个球，里边分别有3个白球，7个黑球。如果我们把这十个球随机分成两份并且放进两个袋子里边，分别是bag1与bag2，假如我们的分配结果是：bag1（黑：3，白：2），bag2：（黑：4，白：1），我们要从中随机挑一个球在bag1的概率是5/10，bag2的概率是5/10，这些就是先验概率，我们可以通过已知数据的频率来求得的。我们在bag1中挑一个白球的概率是多少呢？是2/5，这个就是条件概率，我们可以记为：p（W|bag1）=2/5，p（B|bag1）=3/5，p（W|bag2）=4/5，p（B|bag2）=1/5，这里W代表白球，B代表黑球。假如一个人从那里边挑一个白球，你知道这个球属于bag1与bag2的概率吗？我们要求的这个概率就是后验概率。贝叶斯理论就是为了推测出最大后验概率，来预测当前样本属于哪一类。下边解答下吧：$$p(bag1|W) = \\frac{p(W|bag1)p(bag1)}{p(W)}=\\frac{\\frac{2}{5}\\frac{1}{2}}{\\frac{3}{10}}=\\frac{2}{3}$$ $$p(bag2|W) = \\frac{p(W|bag2)p(bag2)}{p(W)}=\\frac{\\frac{1}{5}\\frac{1}{2}}{\\frac{3}{10}}=\\frac{1}{3}$$ 因为2/3 &gt; 1/3，因此这个球更大可能是属于bag1。 2.获取相关数据​ 从上边的例子大概知道如何利用贝叶斯来预测当前数据样本的概率了吧，我们还以小球为例建立数据集，这次我们再给小球添加一个属性特征，空心或者实心。 序号 类型 颜色 类别（袋子） 1 空 黑 1 2 空 黑 1 3 实 黑 1 4 空 白 1 5 实 白 1 6 空 黑 2 7 空 黑 2 8 实 黑 2 9 实 黑 2 10 实 白 2 3.公式原理假如有两个事件：事件A与事件B先验概率：其实就是每个类别或属性在全体样本中所占的概率，通过观察已知样本数据我们就可以得到。比如：p(bag1)=1/2 p(W) = 3/10 条件概率：在B发生条件下A发生的概率。$$p(A|B) = \\frac{p(AB))}{p(B)}$$ 比如：p(W|bag1)= 2/5 后验概率：在A发生条件下B发生的概率，这里就是通过贝叶斯法则来进行预测。$$p(B|A) = \\frac{p(A|B)*p(B)}{p(A)}=\\frac{p(AB)}{p(A)}$$ 比如：贝叶斯算法剧情介绍中求解p(bag1|W)就是利用贝叶斯来求解。 全概率：假如（A1,A2,A3,…,An）构成完备事件组，对于事件B来说：$$p(B) = \\sum_{i=1}^{n}p(A_{i})p(B|A_{i})=p(A_{1}B)+p(A_{2}B)+p(A_{3}B)+…+p(A_{n}B)$$ ​ 接下来我们将以以上数据集计算，如果有个人这两个袋子中随机选取一个空心白色的小球，应该属于哪个袋子的可能性最大呢？ 我们可以先计算出概率分布：$$p(bag1) = \\frac{1}{2}$$ $$p(bag2) = \\frac{1}{2}$$ $$p(黑|bag1) = \\frac{3}{5}$$ $$p(黑|bag2) = \\frac{3}{5}$$ $$p(白|bag1) = \\frac{2}{5}$$ $$p(白|bag2) = \\frac{1}{5}$$ $$p(空|bag1) = \\frac{3}{5}$$ $$p(空|bag2) = \\frac{2}{5}$$ $$p(实|bag1) = \\frac{2}{5}$$ $$p(实|bag2) = \\frac{3}{5}$$ 这里求得是，这个空心白球来自哪个袋子的概率：$$p(bag1|空,白) = p(空,白|bag1)p(bag1) = \\frac{1}{2}\\frac{3}{5}*\\frac{2}{5}=\\frac{3}{25}$$ $$p(bag2|空,白) = p(空,白|bag2)p(bag2) = \\frac{1}{2}\\frac{2}{5}*\\frac{1}{5}=\\frac{1}{25}$$ 可以看出在bag1中的概率大，则贝叶斯规则就判定这个球属于bag1。需要注意的是，我们这里没有求解p(空,白)是因为两个概率的低都一样，贝叶斯中就不去求解了，其实结果是一样的。 ​ 我们这里所用的是朴素贝叶斯理论，为什么称为朴素呢？是因为我们用了条件独立性假设这个概念。就是样本属性间相互独立，互不关联，这样就会使得样本空间得以减小。举个例子，假如一个数据集有1000个特征，我们要得到很好的概率分布，就要有足够的样本，样本数为N，那么我们要得到N的1000次方个样本才能涵盖所有的可能性。所以我们假设各个属性独立，那么我们就会得到1000N个样本，此时样本数目减小了很多。 4.分析判断​ 我们通过数据集进行训练得到，先验概率与条件概率，我们就可以通过贝叶斯法则求出待预测样本的概率，然后比较不同类别概率的大小，最后选择最大的概率预测的类别就是该样本的类别。 二、算法实践​ 下边的算法实践实现了两个小例子，分别是朴素贝叶斯垃圾邮件过滤与朴素贝叶斯分类器从个人广告中获取区域倾向。 1.朴素贝叶斯垃圾邮件过滤​ 这个例子主要是通过提取邮件文本词向量作为特征，垃圾与非垃圾邮件作为类别进行分类，然后通过贝叶斯法则训练样本数据集得到先验概率P(垃圾)与P(非垃圾)与条件概率P(W|垃圾)P(W|非垃圾)，然后根据新收到的邮件分解为词向量w，来计算P(垃圾|w)与P(非垃圾|w)进行比较。选择最大的就是该邮件的类别，判断时候进行过滤。 ​ 其中代码中需要注意的是，在求解完每个词对应的条件概率后，会出现很多极小的值，如果我们采用乘法就会导致下溢出这个问题，乘法最后的结果特别小可以约等于0，因此我们这里取对数相加和乘法效果是一样的，因为对于f(x)，其中x属于(0,1)，这里f(x)与log(f(x))的单调性是一致的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133import numpy as npimport re'''获取文档中的词汇表'''def createVocabList(dataSet): vocabSet = set([]) # 创建一个空集 for document in dataSet: vocabSet = vocabSet | set(document) # 创建两个集合的并集 return list(vocabSet)'''文档转化为向量vocabList： 词汇列表inputSet： 输入文档集'''def setOfWords2Vec(vocabList,inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] = 1 else: print(\"词： %s 不在我的词汇表！\" % word) return returnVec'''朴素贝叶斯分类器训练函数trainMatrix: 文档矩阵trainCategory： 每篇文档类别标签向量'''def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) #3/6=0.5 # 初始化概率 p0Num = np.ones(numWords) p1Num = np.ones(numWords) p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: # 向量相加 p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 对每个元素做除法 p1Vect = np.log(p1Num/p1Denom) p0Vect = np.log(p0Num/p0Denom) return p0Vect,p1Vect,pAbusive'''朴素贝叶斯分类函数vec2Classify: 要分类的向量p0Vec: 0标签条件概率p1Vec： 1标签条件概率pClass1: 类别1概率'''def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0def testingNB(): listOPosts,listClasses = loadDataSet() myVocabList = createVocabList(listOPosts) trainMat = [] for postinDoc in listOPosts: trainMat.append(setOfWords2Vec(myVocabList,postinDoc)) p0V,p1V,pAb = trainNB0(np.array(trainMat),np.array(listClasses)) testEntry = ['love','my','dalmation'] thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry)) print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb)) testEntry = ['stupid','garbage'] thisDoc = np.array(setOfWords2Vec(myVocabList,testEntry)) print(testEntry,'classified as:',classifyNB(thisDoc,p0V,p1V,pAb))'''文本解析'''def textParse(bigString): listOfTokens = re.split(r'\\W+', bigString) return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2]'''垃圾邮件测试文件'''def spamTest(): docList = [] classList = [] fullText = [] for i in range(1,26): wordList = textParse(open('./data/Ch04/email/spam/%d.txt' % i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList = textParse(open('./data/Ch04/email/ham/%d.txt' % i).read()) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList = createVocabList(docList) trainingSet = list(range(50)) testSet = [] for i in range(10): randIndex = int(np.random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat = [] trainClasses = [] for docIndex in trainingSet: trainMat.append(setOfWords2Vec(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam = trainNB0(np.array(trainMat),np.array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = setOfWords2Vec(vocabList,docList[docIndex]) if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: errorCount += 1 print('分类错误的测试集：',docList[docIndex]) print(\"错误率：\",float(errorCount)/len(testSet))# 垃圾邮件过滤测试spamTest() 2.朴素贝叶斯分类器从个人广告中获取区域倾向​ 本案例是分析在美国两个城市发布征婚信息的时候用词是否有所不同，我们获取这些征婚数据集，然后解析为相应的词向量和对应的类别，这样我们就可以训练出相应的分为训练集与测试机，训练集用来训练出先验概率和条件概率，测试集我们来测试预测准确率。最后我们把这些这两个类型的词大于一定概率阈值的词汇输出，观察个人用词在不同区域的倾向。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153import numpy as npimport reimport feedparserimport operator'''获取文档中的词汇表'''def createVocabList(dataSet): vocabSet = set([]) # 创建一个空集 for document in dataSet: vocabSet = vocabSet | set(document) # 创建两个集合的并集 return list(vocabSet)'''朴素贝叶斯词袋模型'''def bagOfWords2VecMN(vocabList,inputSet): returnVec = [0]*len(vocabList) for word in inputSet: if word in vocabList: returnVec[vocabList.index(word)] += 1 return returnVec'''朴素贝叶斯分类器训练函数trainMatrix: 文档矩阵trainCategory： 每篇文档类别标签向量'''def trainNB0(trainMatrix,trainCategory): numTrainDocs = len(trainMatrix) numWords = len(trainMatrix[0]) pAbusive = sum(trainCategory)/float(numTrainDocs) #3/6=0.5 # 初始化概率 p0Num = np.ones(numWords) p1Num = np.ones(numWords) p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: # 向量相加 p1Num += trainMatrix[i] p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 对每个元素做除法 p1Vect = np.log(p1Num/p1Denom) p0Vect = np.log(p0Num/p0Denom) return p0Vect,p1Vect,pAbusive'''朴素贝叶斯分类函数vec2Classify: 要分类的向量p0Vec: 0标签条件概率p1Vec： 1标签条件概率pClass1: 类别1概率'''def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1): p1 = sum(vec2Classify * p1Vec) + np.log(pClass1) p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1) if p1 &gt; p0: return 1 else: return 0 '''RSS源分类器'''def calcMostFreq(vocabList,fullText): # 计算出现频率 freqDict = &#123;&#125; for token in vocabList: freqDict[token]=fullText.count(token) sortedFreq = sorted(freqDict.items(),key=operator.itemgetter(1),reverse=True) return sortedFreq[:30]'''高频词去除函数'''def localWords(feed1,feed0): docList = [] classList = [] fullText = [] minLen = min(len(feed1['entries']),len(feed0['entries'])) for i in range(minLen): wordList = textParse(feed1['entries'][i]['summary']) docList.append(wordList) fullText.extend(wordList) classList.append(1) wordList = textParse(feed0['entries'][i]['summary']) docList.append(wordList) fullText.extend(wordList) classList.append(0) vocabList = createVocabList(docList) top30Words = calcMostFreq(vocabList,fullText) for pairW in top30Words: if pairW[0] in vocabList:vocabList.remove(pairW[0]) trainingSet = list(range(2*minLen)) testSet = [] for i in range(20): randIndex = int(np.random.uniform(0,len(trainingSet))) testSet.append(trainingSet[randIndex]) del(trainingSet[randIndex]) trainMat = [] trainClasses = [] for docIndex in trainingSet: trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex])) trainClasses.append(classList[docIndex]) p0V,p1V,pSpam = trainNB0(np.array(trainMat),np.array(trainClasses)) errorCount = 0 for docIndex in testSet: wordVector = bagOfWords2VecMN(vocabList,docList[docIndex]) if classifyNB(np.array(wordVector),p0V,p1V,pSpam) != classList[docIndex]: errorCount += 1 print(\"错误率是：\",float(errorCount)/len(testSet)) print('分类错误的测试集：',docList[docIndex]) return vocabList,p0V,p1V'''最具有表征性的词汇显示'''def getTopWords(ny,sf): vocabList,p0V,p1V = localWords(ny,sf) topNY = [] topSF = [] for i in range(len(p0V)): if p0V[i] &gt; -0.6 : topSF.append((vocabList[i]),p0V[i]) if p1V[i] &gt; -0.6 : topNY.append(vocabList[i],p1V) sortedSF = sorted(topSF,key=lambda pair:pair[1],reverse=True) print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\") for item in sortedSF: print(item[0]) sortedNY = sorted(topNY,key=lambda pair:pair[1],reverse=True) print(\"NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*NF*\") for item in sortedNY: print(item[0]) # 从个人广告中获取区域倾向ny = feedparser.parse('http://www.nasa.gov/rss/dyn/image_of_the_day.rss')sf = feedparser.parse('http://rss.tom.com/happy/happy.xml')vocabList,pSF,pNY = localWords(ny,sf)getTopWords(ny,sf) 三、后记​ 这里我们主要说了朴素贝叶斯理论，其实还有好多，半朴素贝叶斯理论等，下一篇主要介绍些逻辑回归。","categories":[],"tags":[]},{"title":"决策树（Decision Tree）","slug":"DT","date":"2019-03-09T16:00:00.000Z","updated":"2019-04-15T10:24:40.044Z","comments":true,"path":"2019/03/10/DT/","link":"","permalink":"http://yoursite.com/2019/03/10/DT/","excerpt":"","text":"一、算法理论1.DT算法剧情介绍​ 决策树，可以简单理解为数据结构中的N叉树，可以有好多个分支，但是决策树又和数据结构中的N叉树有什么不同呢？当然了，不然为什么称之为一个棒棒的算法呢。我们还是以一个例子来讲解，这样的话会更加清晰。 ​ 假如你去瓜农那里买西瓜，如何判断西瓜的好坏呢？首先我们考虑下人是如何考虑做出决策挑选出好的西瓜呢。瓜农一般看下色泽是青绿，再看下根蒂是蜷缩，然后又敲了下西瓜敲声沉闷，好像瓜农已经有了眉目，又看了看纹理是清晰，脐部凹陷，又摸了摸西瓜硬滑，点了点头说，这是好瓜。你可能有时还会看到，瓜农看了看色泽，敲了敲西瓜，看看根蒂就说这是好瓜。这些都没有错，瓜农是这样判断的，我们的决策树貌似也可以呀，每次判断就像树的分支一样，然后瓜农可能感觉色泽比较重要，先看了这个，然后下边觉得根蒂次重要，依次看下去，最终得出结论。我们是不是可以把每次判断作为树的分支呢，然后那么多我们到底怎么去做出决定先看那个特征呢？是的决策树引入了信息论里的熵和信息增益，这就是判断标准，好了，一颗树就这样种起来了。 2.获取相关数据​ 我们依据瓜农现有的经验获取一些数据，假设这些数据都是判断正确的训练数据。下边是数据集： 编号 色泽 根蒂 敲声 纹理 脐部 触感 好瓜 1 青绿 蜷缩 浊响 清晰 凹陷 硬滑 是 2 乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 是 3 乌黑 蜷缩 浊响 清晰 凹陷 硬滑 是 4 青绿 蜷缩 沉闷 清晰 凹陷 硬滑 是 5 浅白 蜷缩 浊响 清晰 凹陷 硬滑 是 6 青绿 稍蜷 浊响 清晰 稍凹 软粘 是 7 乌黑 稍蜷 浊响 稍糊 稍凹 软粘 是 8 乌黑 稍蜷 浊响 清晰 稍凹 硬滑 是 9 乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 否 10 青绿 硬挺 清脆 清晰 平坦 软粘 否 11 浅白 硬挺 清脆 模糊 平坦 硬滑 否 12 浅白 蜷缩 浊响 模糊 平坦 软粘 否 13 青绿 稍蜷 浊响 稍糊 凹陷 硬滑 否 14 浅白 稍蜷 沉闷 稍糊 凹陷 硬滑 否 15 乌黑 稍蜷 浊响 清晰 稍凹 软粘 否 16 浅白 蜷缩 浊响 模糊 平坦 硬滑 否 17 青绿 蜷缩 沉闷 稍糊 稍凹 硬滑 否 3.公式原理​ 在我们判断之前肯定先要用上边那个特征先来进行划分，我们这就需要用信息熵的计算了，信息熵是衡量当前信息的纯度，我们的目标是将不纯的数据尽快变得更加纯正，所以要找到熵值降低最快的特征进行划分。 ​ 假设当前样本集合D的第k类样本所占比例为Pk（k=1,2,3,…,|y|），D的信息熵定义为：$$Ent(D)=-\\sum_{k=1}^{|y|}p_{k}\\log_{2}p_{k}$$其中Ent（D）越小，纯度越高。 ​ 假如离散特征有a个有|V|个可能的取值{a1,a2,…,av},我们按照这个特征进行划分，会划分|V|个子集。得到的信息增益值为：$$Gain(D,a)=Ent(D)-\\sum_{v=1}^{|V|}\\frac{|D^v|}{|D|}Ent(D^{v})$$其中|Dv|指的是当前第v个分枝接点，包含了所有在特征a上取值为av的样本个数。Gain(D,a)指的是信息增益，我们应该选择信息增益最大特征来进行划分。 ​ 接下来我们计算下信息增益，看下到底是如何进行节点划分计算的，没划分前，所有样本都属于根节点，此时的信息熵为好瓜8个，不是好瓜的有9个，并且|y|=2是2分类，所以 p1=8/17，p2=9/17 。代入公式计算根节点的信息熵：$$Ent(D)=-\\sum_{k=1}^{2}p_{k}\\log_{2}p_{k}=-\\left (\\frac{8}{17}\\log_2\\frac{8}{17}+\\frac{9}{17}\\log_2\\frac{9}{17} \\right )=0.998$$​ 然后我们就要按照特征划分了，特征集为{色泽，根蒂，敲声，纹理，脐部，触感}，我们要计算每个特征划分的信息增益。假如我们选择了 色泽 这个特征，然后特征值有3个{青绿，乌黑，浅白}，此时按照这个特征划分会得到3个子集：D1={色泽=青绿}，D2={色泽=乌黑}，D3={色泽=浅白}。然后我们可以看下上边的数据集，其中D1={1,4,6,10,13,17}，D2={2,3,7,8,9,15}，D3={5,11,12,14,16}，分别有6，6，5个样例，则在D1中，正例：p1=3/6，反例：p2=3/6。在D2中，正例：p1=4/6，反例：p2=2/6。在D3中，正例：p1=1/5，反例：4/5。我们可以计算出3个分支节点的信息熵：$$Ent(D^{1})=-\\left (\\frac{3}{6}\\log_2\\frac{3}{6}+\\frac{3}{6}\\log_2\\frac{3}{6} \\right )=1.000$$ $$Ent(D^{2})=-\\left (\\frac{4}{6}\\log_2\\frac{4}{6}+\\frac{2}{6}\\log_2\\frac{2}{6} \\right )=0.918$$ $$Ent(D^{3})=-\\left (\\frac{1}{5}\\log_2\\frac{1}{5}+\\frac{4}{5}\\log_2\\frac{4}{5} \\right )=0.722$$ 信息增益为：$$Gain(D,色泽)=Ent(D)-\\sum_{v=1}^{3}\\frac{|D^v|}{|D|}Ent(D^{v})=$$ $$0.998-\\left (\\frac{6}{17}\\times1.000 +\\frac{6}{17}\\times0.918 +\\frac{6}{17}\\times0.722 \\right )=0.109$$ 以此类推，我们可以求出其他几个特征的信息增益：$$Gain(D,根蒂)=0.143$$ $$Gain(D,敲声)=0.141$$ $$Gain(D,纹理)=0.381$$ $$Gain(D,脐部)=0.289$$ $$Gain(D,触感)=0.006$$ ​ 此时我们可以看出纹理特征的信息增益最大，所以我们先选择纹理特征进行划分，依次按照这样的方式进行下去，最终我们划分到一个节点子集时，里边的样本全部属于同一类，则此时停止划分，这个节点就是决策树的叶节点。这就是决策树与普通树的区别，节点划分方式不同。这个就是著名的 ID3 决策树算法。 4.分析预测​ 通过样本进行训练，我们可以得到一棵树，我们把树存储在本地，然后我们输入一个含有这六个特征的西瓜特征值，就可以判断是好瓜还是不好的瓜了。 二、算法实践​ 接下来是一个算法关于预测佩戴隐形眼镜类型的一个案例。 决策树预测佩戴隐形眼镜类型​ 医院眼科医生要判断一个人适合带什么类型镜片的隐形眼镜，其中包括硬材质、软材质、不适合佩戴者三个类型。判断一个人适合什么类型需要四个特征，分别是：年纪，处方，散光，流泪比率。 代码实战： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221from math import logimport operatorimport matplotlib.pyplot as pltimport pickle'''计算香农熵'''def calcShannonEnt(dataSet): numEntries = len(dataSet) labelCounts = &#123;&#125; # 为所有可能分类创建字典 for featVec in dataSet: currentLabel = featVec[-1] if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 shannonEnt = 0.0 for key in labelCounts: prob = float(labelCounts[key])/numEntries shannonEnt -= prob*log(prob,2) # 计算香农熵 return shannonEnt'''给定特征划分数据集dataSet: 待划分数据集axis: 划分数据集的特征value： 特征的返回值知识补充：1.extend例如：a = [1,2,3] b = [4,5,6] a.extend(b) output:[1,2,3,4,5,6]1.append例如：a = [1,2,3] b = [4,5,6] a.append(b) output:[1,2,3,[4,5,6]] '''def splitDataSet(dataSet,axis,value): retDataSet = [] for featVec in dataSet: if featVec[axis] == value: reducedFeatVec = featVec[:axis] reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSet'''选择最好的数据集划分方式'''def chooseBestFeatureToSplit(dataSet): numFeatures = len(dataSet[0]) - 1 baseEntropy = calcShannonEnt(dataSet) bestInfoGain = 0.0 bestFeature = -1 for i in range(numFeatures): featList = [example[i] for example in dataSet] uniqueVals = set(featList) # 集合数据结构中，每个值互不相同 newEntropy = 0.0 # 计算每种划分的信息熵 for value in uniqueVals: subDataSet = splitDataSet(dataSet,i,value) prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob*calcShannonEnt(subDataSet) # 计算最好的信息增益 infoGain = baseEntropy - newEntropy if (infoGain &gt; bestInfoGain): bestInfoGain = infoGain bestFeature = i return bestFeature'''无法决定分类时采用多数表决法分类'''def majorityCnt(classList): classCount = &#123;&#125; for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0]'''创建树'''def createTree(dataSet,labels): classList = [example[-1] for example in dataSet] # 类别相同则停止继续划分 if classList.count(classList[0]) == len(classList): return classList[0] # 遍历完所有特征返回时返回出现次数最多的 if len(dataSet[0]) == 1: return majorityCnt(classList) bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] myTree = &#123;bestFeatLabel:&#123;&#125;&#125; del(labels[bestFeat]) # 得到列表包含的所有属性值 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels) return myTree'''获取叶节点的数目'''def getNumLeafs(myTree): numLeafs = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': numLeafs += getNumLeafs(secondDict[key]) else: numLeafs += 1 return numLeafs'''获取树的层数'''def getTreeDepth(myTree): maxDepth = 0 firstStr = list(myTree.keys())[0] secondDict = myTree[firstStr] for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': thisDepth = 1+getTreeDepth(secondDict[key]) else: thisDepth = 1 if thisDepth &gt; maxDepth:maxDepth = thisDepth return maxDepth'''绘制树函数'''decisionNode = dict(boxstyle=\"sawtooth\",fc=\"0.8\")leafNode = dict(boxstyle=\"round4\",fc=\"0.8\")arrow_args = dict(arrowstyle=\"&lt;-\")def plotMidText(cntrPt,parentPt,txtString): xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0] yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1] createPlot.ax1.text(xMid,yMid,txtString)def plotTree(myTree,parentPt,nodeTxt): numLeafs = getNumLeafs(myTree) depth = getTreeDepth(myTree) firstStr = list(myTree.keys())[0] cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW,plotTree.yOff) plotMidText(cntrPt,parentPt,nodeTxt) plotNode(firstStr,cntrPt,parentPt,decisionNode) secondDict = myTree[firstStr] plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD for key in secondDict.keys(): if type(secondDict[key]).__name__ == 'dict': plotTree(secondDict[key],cntrPt,str(key)) else: plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW plotNode(secondDict[key],(plotTree.xOff,plotTree.yOff),cntrPt,leafNode) plotMidText((plotTree.xOff,plotTree.yOff),cntrPt,str(key)) plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalDdef plotNode(nodeTxt,centerPt,parentPt,nodeType): createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords='axes fraction', xytext=centerPt,textcoords='axes fraction', va=\"center\",ha=\"center\",bbox=nodeType,arrowprops=arrow_args)def createPlot(inTree): fig = plt.figure(1,facecolor='white') fig.clf() axprops = dict(xticks=[],yticks=[]) createPlot.ax1 = plt.subplot(111,frameon=False,**axprops) plotTree.totalW = float(getNumLeafs(inTree)) plotTree.totalD = float(getTreeDepth(inTree)) plotTree.xOff = -0.5/plotTree.totalW plotTree.yOff = 1.0 plotTree(inTree,(0.5,1.0),'') plt.show()'''决策树分类'''def classify(inputTree,featLabels,testVec): firstStr = inputTree.keys()[0] secondDict = inputTree[firstStr] featIndex = featLabels.index(firstStr) for key in secondDict.keys(): if testVec[featIndex] == key: if type(secondDict[key]).__name__ == 'dict': classLabel = classify(secondDict[key],featLabels,testVec) else: classLabel = secondDict[key] return classLabel'''存储决策树'''def storeTree(inputTree,filename): fw = open(filename,'w') pickle.dump(inputTree,fw) fw.close()def grabTree(filename): fr = open(filename) return pickle.load(fr)# 决策树预测隐形眼镜类型案例fr = open('./data/Ch03/lenses.txt')lenses = [inst.strip().split('\\t') for inst in fr.readlines()]lensesLabels = ['age','prescript','astigmatic','tearRate']lensesTree = createTree(lenses,lensesLabels)createPlot(lensesTree) 三、后记​ 这里只说了ID3决策树，其实ID3是有BUG的，因为这个算法更偏向先去选择属性值含有量多的，在C4.5中就采用信息增益比来解决这个问题。其实还有过拟合问题，我们也可以采用预剪枝与和后剪枝来处理过拟合问题，这个后续进行补充。下一篇将介绍贝叶斯分类器。","categories":[],"tags":[]},{"title":"KNN（K-Nearest Neighbor）","slug":"KNN","date":"2019-03-08T16:00:00.000Z","updated":"2019-04-22T08:06:58.894Z","comments":true,"path":"2019/03/09/KNN/","link":"","permalink":"http://yoursite.com/2019/03/09/KNN/","excerpt":"","text":"一、算法理论1.KNN算法剧情介绍​ KNN算法其实是采用以距离为度量单位来进行对待测样本进行分类，那么距离如何测量呢？这个k又是指的是什么呢？采用一个例子来进行讲解或许能够更加清晰。 ​ 假如一个电影网站要对当前的一部电影进行评判是爱情片还是动作片，其实我们人看的时候会怎么想呢？其实是这样的，我们可能看到接吻，拥抱等动作较多的时候认为这是爱情片，武打，枪战等动作较多的时候认为这是动作片。So，电影网站后台的算法会去如何做出判断呢？总该不会用人吧，累死。此时KNN算法就派上用场了，假如我们提取其中的两个特征：接吻次数和武打次数，虽然爱情片和动作片中都会出现接吻和武打，但是爱情片肯定接吻次数多鸭，动作片肯定武打次数多鸭。 2.获取相关数据​ 我们确定了这两个特征后，假如我们得到如下数据集，实际中没有接吻、武打没那么多次，这里只是举例。 电影名称 接吻 武打 类型 电影1 3 67 动作片 电影2 5 45 动作片 电影3 55 2 爱情片 电影4 68 8 爱情片 电影5 66 5 爱情片 3.公式原理​ 我们该如何计算举例呢？一般我们采用欧氏距离来计算。假如一篇电影要判断是什么类型的，待测电影（接吻：60次，武打：5次）。存在有（x1,y1）与（x2,y2），这里的x与y分别指的是接吻与武打这两个特征。 欧氏距离公式：$$d=\\sqrt{\\left ( x1-x2 \\right )^{2}+\\left ( y1-y2 \\right ) ^{2}}$$​ 这里待测电影与上边5个电影都计算一下欧氏距离。计算之后分别是：d1=84，d2=68，d3=6，d4=9，d5=6。这样就得出未知电影与样本集中每个电影的距离了。 4.分析判断​ 现在我们要判断下这不未知电影属于那种类型了，所以这里我们可以解释一下KNN中的K了，其实我们计算完未知电影与每个样本电影的欧氏距离后，需要进行升序排序，此时为d3，d5，d4，d2，d1。我们选取前三个距离作为参考，其中这里K=3，这就是KNN中的K指代的意思，然后我们判断在这三个钟属于爱情片的个数与属于武打片的个数。此时爱情片的个数为3，所以这部未知电影肯定是爱情片了。就是这样，我们就可以通过KNN分析判断一个电影的类型了。 二、算法实践​ 接下来，有两个实践的小demo，分别是约会网站与手写识别系统。 1.KNN约会网站配对​ 假如一个人在约会网站上要寻找自己心仪的那个Ta，但是看了好多都不合适，我们的KNN算法可以帮助Ta判断一下，是不喜欢的人、魅力一般的人还是非常有魅力的人。我们的判断特征有三个，分别是每周消费冰激凌的公升数、玩游戏所耗占时间百分比与每年获取的飞行常客旅程数。数据集(大约有1000个标注数据，我们用10%做测试，90%来测距离用，K=3)格式大致如下： 人员 每周消费冰激凌的公升数 玩游戏所耗占时间百分比 每年获取的飞行常客旅程数 类型 p1 0.95 8.32 40902 极具魅力 p2 1.67 7.15 14488 魅力一般 p3 0.80 1.44 26052 不喜欢 … … … … … ​ 这里要强调下数据归一化，因为飞行旅程数在计算欧氏距离时比重太大，然而三个特征的权重都是等价的，不会说我看Ta时更关注某项特征（实际上是这样的，但是这里一律采用公平对待），所以我们要进行归一化： 归一化公式：$$newVal = (oldVal - min)\\div (max-min)$$oldVal是原始属性值，min是特征列最小值，max是特征列最大值，newVal是归一化后的值。 下边就是实战的代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124import operatorfrom numpy import *'''近邻分类参数： inX： 要待测的样本dataSet： 获取的样本特征集labels: 样本数据标签K: 要选取前k个与待测样本最近距离且已有标签的样本数据''''''numpy 知识补充：1.shape[0] 指的是读取第一维矩阵的长度例如： np.shape([[1,1],[2,2],[3,3]]) output: (3L,2L) 其中shape[0] 就是指的矩阵有多少行，即3L2.tile 一般是在列或者行方向上重复复制几次例如： np.tile([0,0],(2,1)) output: array([[0,0], [0,0]]) 指的是从行方向上重复复制2次，列1次3.sum 一般是对矩阵一行或者一列求和例如： np.sum([[0,1,2],[2,1,3]],axis=1) output:array([3,6]) axis=1指的是对矩阵的行进行求和4.argsort 将数组中的元素从小到大排列，返回对应数值的索引例如： x = np.array([1,4,3,-1,6,9]) x.argsort() output: array([3,0,2,1,4,5]) 返回对应数值的索引5.operator.itemgetter(1) 获取对象第一个域的值例如： x = np.array([1,2,3,4,5]) b = operator.itemgetter(1) b(x) output: 2 下边用法是按照第二列值进行降序排序'''def classify0(inX,dataSet,labels,K): dataSetSize = dataSet.shape[0] # 距离计算 diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() classCount = &#123;&#125; # 选择最小距离的k个点 for i in range(K): votelabel = labels[sortedDistIndicies[i]] classCount[votelabel] = classCount.get(votelabel,0) + 1 # 排序 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0]'''读取文本数据，转化为numpy矩阵'''def file2matrix(filename): fr = open(filename) arrayOLines = fr.readlines() numberOfLines = len(arrayOLines) # 得到文件行数 returnMat = zeros((numberOfLines,3)) classLabelVector = [] index = 0 # 解析文件数据到列表 for line in arrayOLines: line = line.strip() # 去掉回车 listFormLine = line.split('\\t') returnMat[index,:] = listFormLine[0:3] classLabelVector.append(int(listFormLine[-1])) index += 1 return returnMat,classLabelVector'''归一化特征值'''def autoNorm(dataSet): minVals = dataSet.min(0) maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals,(m,1)) normDataSet = normDataSet/tile(ranges,(m,1)) return normDataSet,ranges,minVals '''测试代码'''def datingClassTest(): hoRatio = 0.1 datingDataMat,datingLabels = file2matrix('./data/Ch02/datingTestSet2.txt') normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*hoRatio) errorCount = 0 for i in range(numTestVecs): classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3) print(\"返回分类结果是：%d,真正类别是：%d\" % (classifierResult , datingLabels[i])) if (classifierResult != datingLabels[i]): errorCount += 1 print(\"错误率是：%f\" % (errorCount/float(numTestVecs))) '''约会网站预测'''def classifyPerson(): resultList = ['讨厌','一般','非常喜欢'] percentTats = float(input(\"花费玩视频游戏占用的比例？\")) ffMiles = float(input(\"每年飞行里程数？\")) iceCream = float(input(\"每周消费冰激凌的公升数？\")) datingDataMat,datingLabels = file2matrix('./data/Ch02/datingTestSet2.txt') normMat,ranges,minVals = autoNorm(datingDataMat) inArr = array([ffMiles,percentTats,iceCream]) classifierResult = classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print(\"你或许对这种人：\",resultList[classifierResult-1]) classifyPerson() 2.KNN判断手写识别​ 我们有好多张数字图片，然后转化成32x32的灰白格式就是用0与1来表示，可能会想到怎么计算欧氏距离呢，我们就把32x32的矩阵转化为1x1024的向量，然后就可以计算了。这里我们取K=3。 代码实战： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import operatorfrom numpy import *import matplotlibimport matplotlib.pyplot as pltfrom os import listdir'''分类函数'''def classify0(inX,dataSet,labels,K): dataSetSize = dataSet.shape[0] # 距离计算 diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() classCount = &#123;&#125; # 选择最小距离的k个点 for i in range(K): votelabel = labels[sortedDistIndicies[i]] classCount[votelabel] = classCount.get(votelabel,0) + 1 # 排序 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) return sortedClassCount[0][0]'''手写识别系统图像转向量'''def img2vector(filename): returnVect = zeros((1,1024)) fr = open(filename) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int(lineStr[j]) return returnVect'''手写数字识别系统测试代码'''def handwritingClassTest(): hwLabels = [] trainingFileList = listdir('./data/Ch02/digits/trainingDigits') # 获取目录内容 m = len(trainingFileList) trainingMat = zeros((m,1024)) for i in range(m): fileNameStr = trainingFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) hwLabels.append(classNumStr) trainingMat[i:] = img2vector('./data/Ch02/digits/trainingDigits/%s' % fileNameStr) testFileList = listdir('./data/Ch02/digits/testDigits') errorCount = 0 mTest = len(testFileList) for i in range(mTest): fileNameStr = testFileList[i] fileStr = fileNameStr.split('.')[0] classNumStr = int(fileStr.split('_')[0]) vectorUnderTest = img2vector('./data/Ch02/digits/testDigits/%s' % fileNameStr) classifierResult = classify0(vectorUnderTest,trainingMat,hwLabels,3) print(\"分类结果是：%d,真正结果是：%d\" % (classifierResult,classNumStr)) if (classifierResult != classNumStr): errorCount += 1 print(\"分类错误数目是:%d\" % errorCount) print(\"错误率：%f\" % float(errorCount/mTest)) 三、后记​ 纸上得来终觉浅，绝知此事要躬行。下边会说以下决策树的故事。","categories":[],"tags":[]},{"title":"markdown常用语法","slug":"markdown基本用法","date":"2019-03-07T15:39:00.000Z","updated":"2019-03-07T15:54:20.944Z","comments":true,"path":"2019/03/07/markdown基本用法/","link":"","permalink":"http://yoursite.com/2019/03/07/markdown基本用法/","excerpt":"","text":"​ 好久没有写博客了，这次用github搭建一个个人博客，之前一直用csdn。看着网上的帖子写一下这个markdown的基本用法吧，省的下次想不起来还要去查人家的，确实有些不方便。 1.生成头部​ 开启一个md文件后，输入 “—+回车” 就可以写这边文章的标题（title:这里要打一个空格），日期（date: 同理空格），标识（tags: 空格） 2.快捷键撤销：Crtl+Z 重做：Crtl+Y 加粗：Crtl+B 斜体：Crtl+I 标题：Crtl+Shift+H 插入代码：Crtl+Shift+K 插入链接：Crtl+Shift+L 插入图片：Crtl+Shift+G 六个等级标题：Ctrl+1，2,3,4,5,6 3.创建标题​ 我第一次用也不知道如何去生成这些字，后来看到了帖子上说： “#+空格“ 就可以就可以显示一级标题h1(这个是HTML中好h1字号的意思，比较大)，然后可以再小一些的标题，总共有六个级别。 4.后记​ 其实还有好多用法，基本在客户端中都有，段落里边，或者格式里边，直接选择或者快捷键都挺快的，很方便。 ​ 后期一直更新算法这块的内容吧，基础算法，机器学习，深度学习等方面的内容。","categories":[],"tags":[{"name":"生成头部，快捷键，标题创建","slug":"生成头部，快捷键，标题创建","permalink":"http://yoursite.com/tags/生成头部，快捷键，标题创建/"}]}]}